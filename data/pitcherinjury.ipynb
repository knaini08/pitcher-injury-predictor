{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e5dede-f447-4a12-9faa-a19928ecc9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "P_BASE = \"data/mlb_pitch_data_2021_2023.csv\"\n",
    "\n",
    "pitches = pd.read_csv(P_BASE, low_memory=False)\n",
    "pitches['game_year'] = pitches['game_year'].astype(int)\n",
    "\n",
    "print(\"Rows by year:\")\n",
    "print(pitches['game_year'].value_counts().sort_index())\n",
    "\n",
    "if 'game_date' in pitches.columns:\n",
    "    pitches['game_date'] = pd.to_datetime(pitches['game_date'], errors='coerce')\n",
    "    by_month_23 = (pitches[pitches['game_year']==2023]\n",
    "                   .groupby(pitches['game_date'].dt.month).size().sort_index())\n",
    "    print(\"\\n2023 rows by month:\")\n",
    "    print(by_month_23)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5188f11-9a87-4a1c-a495-c11408a24bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "P_BASE = \"mlb_pitch_data_2021_2023.csv\"\n",
    "\n",
    "pitches = pd.read_csv(P_BASE, low_memory=False)\n",
    "pitches['game_year'] = pitches['game_year'].astype(int)\n",
    "\n",
    "print(\"Rows by year:\")\n",
    "print(pitches['game_year'].value_counts().sort_index())\n",
    "\n",
    "if 'game_date' in pitches.columns:\n",
    "    pitches['game_date'] = pd.to_datetime(pitches['game_date'], errors='coerce')\n",
    "    by_month_23 = (pitches[pitches['game_year']==2023]\n",
    "                   .groupby(pitches['game_date'].dt.month).size().sort_index())\n",
    "    print(\"\\n2023 rows by month:\")\n",
    "    print(by_month_23)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7235393e-b04f-4d84-be98-a2aa7925a251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from pybaseball import statcast\n",
    "\n",
    "P = \"mlb_pitch_data_2021_2023.csv\"\n",
    "THRESHOLD = 10_000",
    "\n",
    "def month_bounds(year:int, month:int):\n",
    "    start = datetime(year, month, 1)\n",
    "    nxt   = (start.replace(day=28) + timedelta(days=4)).replace(day=1)\n",
    "    end   = nxt - timedelta(days=1)\n",
    "    return start, end\n",
    "\n",
    "def week_ranges(start_dt:datetime, end_dt:datetime):\n",
    "    cur = start_dt\n",
    "    out = []\n",
    "    while cur <= end_dt:\n",
    "        wk_end = min(cur + timedelta(days=6), end_dt)\n",
    "        out.append((cur.strftime(\"%Y-%m-%d\"), wk_end.strftime(\"%Y-%m-%d\")))\n",
    "        cur = wk_end + timedelta(days=1)\n",
    "    return out\n",
    "\n",
    "def fetch_range(s, e):\n",
    "    try:\n",
    "        df = statcast(s, e)\n",
    "        if df is None or len(df) == 0:\n",
    "            return pd.DataFrame()\n",
    "        return df\n",
    "    except Exception:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def fetch_month_with_retries(year:int, month:int):\n",
    "    s_dt, e_dt = month_bounds(year, month)\n",
    "    full = fetch_range(s_dt.strftime(\"%Y-%m-%d\"), e_dt.strftime(\"%Y-%m-%d\"))\n",
    "    if len(full):\n",
    "        return full[full.get(\"game_year\", year) == year]\n",
    "\n",
    "    mid = s_dt + timedelta(days=(e_dt - s_dt).days // 2)\n",
    "    halves = []\n",
    "    for s,e in [(s_dt, mid), (mid + timedelta(days=1), e_dt)]:\n",
    "        part = fetch_range(s.strftime(\"%Y-%m-%d\"), e.strftime(\"%Y-%m-%d\"))\n",
    "        if len(part):\n",
    "            halves.append(part)\n",
    "    if halves:\n",
    "        out = pd.concat(halves, ignore_index=True)\n",
    "        return out[out.get(\"game_year\", year) == year]\n",
    "\n",
    "    weeks = []\n",
    "    for s_str, e_str in week_ranges(s_dt, e_dt):\n",
    "        part = fetch_range(s_str, e_str)\n",
    "        if len(part):\n",
    "            weeks.append(part)\n",
    "    if weeks:\n",
    "        out = pd.concat(weeks, ignore_index=True)\n",
    "        return out[out.get(\"game_year\", year) == year]\n",
    "\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def dedupe_by_pitch(df):\n",
    "    keys = [\"game_pk\",\"at_bat_number\",\"pitch_number\"]\n",
    "    if all(k in df.columns for k in keys):\n",
    "        before = len(df)\n",
    "        df = df.drop_duplicates(subset=keys)\n",
    "        after  = len(df)\n",
    "        if after != before:\n",
    "            print(f\"  De-duplicated: {before:,} â†’ {after:,}\")\n",
    "    return df\n",
    "\n",
    "p = pd.read_csv(P, low_memory=False)\n",
    "p[\"game_year\"] = pd.to_numeric(p[\"game_year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "p[\"game_date\"] = pd.to_datetime(p.get(\"game_date\"), errors=\"coerce\")\n",
    "\n",
    "print(\"Before (by year):\")\n",
    "print(p[\"game_year\"].value_counts().sort_index())\n",
    "\n",
    "p23 = p[p[\"game_year\"] == 2023].copy()\n",
    "month_counts = p23.groupby(p23[\"game_date\"].dt.month).size().to_dict()\n",
    "months = list(range(3, 11)) ",
    "todo = [m for m in months if month_counts.get(m, 0) < THRESHOLD]\n",
    "\n",
    "print(\"\\n2023 month counts before:\", month_counts)\n",
    "print(\"Months to fetch (threshold < 10k rows):\", todo)\n",
    "\n",
    "for m in todo:\n",
    "    print(f\"\\nFetching 2023-{m:02d} ...\")\n",
    "    dfm = fetch_month_with_retries(2023, m)\n",
    "    print(f\"  fetched rows: {len(dfm):,}\")\n",
    "    if len(dfm) == 0:\n",
    "        print(\"  WARNING: still empty; try re-running later or lowering date window.\")\n",
    "        continue\n",
    "    dfm = dfm[dfm.get(\"game_year\", 2023) == 2023]\n",
    "    p = pd.concat([p, dfm], ignore_index=True)\n",
    "    p = dedupe_by_pitch(p)\n",
    "    p.to_csv(P, index=False)\n",
    "\n",
    "    p = pd.read_csv(P, low_memory=False)\n",
    "    p[\"game_year\"] = pd.to_numeric(p[\"game_year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    p[\"game_date\"] = pd.to_datetime(p.get(\"game_date\"), errors=\"coerce\")\n",
    "    p23 = p[p[\"game_year\"] == 2023]\n",
    "    month_counts = p23.groupby(p23[\"game_date\"].dt.month).size().to_dict()\n",
    "    print(\"  Updated 2023 month counts:\", month_counts)\n",
    "\n",
    "print(\"\\nFinal (by year):\")\n",
    "print(p[\"game_year\"].value_counts().sort_index())\n",
    "print(\"Final 2023 month counts:\", p[p[\"game_year\"]==2023].groupby(p[\"game_date\"].dt.month).size().to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025f1b63-4bac-4546-9320-ca0fbe31b922",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PITCHES_PATH = \"mlb_pitch_data_2021_2023.csv\"\n",
    "INJURY_PATH  = \"rosterresource_injuries_cleaned.csv\"\n",
    "\n",
    "pitches = pd.read_csv(PITCHES_PATH, low_memory=False)\n",
    "pitches['game_year'] = pd.to_numeric(pitches['game_year'], errors='coerce').astype('Int64')\n",
    "\n",
    "inj = pd.read_csv(INJURY_PATH)\n",
    "\n",
    "keep = [c for c in [\"MLBAMID\",\"Injury / Surgery Date\",\"Injury / Surgery\"] if c in inj.columns]\n",
    "inj = inj[keep].copy()\n",
    "\n",
    "pitches['pitcher'] = pd.to_numeric(pitches['pitcher'], errors='coerce').astype('Int64')\n",
    "inj['MLBAMID']     = pd.to_numeric(inj['MLBAMID'], errors='coerce').astype('Int64')\n",
    "\n",
    "merged = pitches.merge(inj, left_on='pitcher', right_on='MLBAMID', how='left')\n",
    "\n",
    "merged['injured'] = merged['Injury / Surgery'].notna().astype(int)\n",
    "\n",
    "merged.to_csv(\"data/pitches_with_injuries_2021_2023.csv\", index=False)\n",
    "print(\"Merged file saved.\")\n",
    "print(\"Year counts:\\n\", merged['game_year'].value_counts().sort_index())\n",
    "print(\"Injury flags:\\n\", merged['injured'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31309acc-e295-4cb8-9ec1-1138fde37a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "PITCHES_PATH = \"mlb_pitch_data_2021_2023.csv\"\n",
    "INJURY_PATH  = \"rosterresource_injuries_cleaned.csv\"\n",
    "\n",
    "pitches = pd.read_csv(PITCHES_PATH, low_memory=False)\n",
    "pitches['game_year'] = pd.to_numeric(pitches['game_year'], errors='coerce').astype('Int64')\n",
    "\n",
    "inj = pd.read_csv(INJURY_PATH)\n",
    "\n",
    "keep = [c for c in [\"MLBAMID\",\"Injury / Surgery Date\",\"Injury / Surgery\"] if c in inj.columns]\n",
    "inj = inj[keep].copy()\n",
    "\n",
    "pitches['pitcher'] = pd.to_numeric(pitches['pitcher'], errors='coerce').astype('Int64')\n",
    "inj['MLBAMID']     = pd.to_numeric(inj['MLBAMID'], errors='coerce').astype('Int64')\n",
    "\n",
    "merged = pitches.merge(inj, left_on='pitcher', right_on='MLBAMID', how='left')\n",
    "\n",
    "merged['injured'] = merged['Injury / Surgery'].notna().astype(int)\n",
    "\n",
    "merged.to_csv(\"pitches_with_injuries_2021_2023.csv\", index=False)\n",
    "print(\"Merged file saved.\")\n",
    "print(\"Year counts:\\n\", merged['game_year'].value_counts().sort_index())\n",
    "print(\"Injury flags:\\n\", merged['injured'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99395697-3d1e-4bae-86aa-45776832dd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd() if (Path.cwd()/ \"data\").exists() else (Path.cwd()/ \"pitcher-injury-predictor\")\n",
    "DATA = ROOT / \"data\"\n",
    "DATA.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "p_main   = DATA / \"mlb_pitch_data_2021_2023.csv\"\n",
    "p_merged = ROOT / \"pitches_with_injuries_2021_2023.csv\"         ",
    "p_gz_out = DATA / \"pitches_with_injuries_2021_2023.csv.gz\"      ",
    "p_gz_in  = DATA / \"pitches_with_injuries_2021_2023.csv.gz\"      \n",
    "p_inj    = DATA / \"rosterresource_injuries_2023.csv\"\n",
    "\n",
    "def load_statcast():\n",
    "    df = pd.read_csv(p_main, low_memory=False)\n",
    "    if \"game_date\" in df.columns:\n",
    "        df[\"game_date\"] = pd.to_datetime(df[\"game_date\"], errors=\"coerce\")\n",
    "    if \"game_year\" not in df.columns and \"game_date\" in df.columns:\n",
    "        df[\"game_year\"] = df[\"game_date\"].dt.year\n",
    "    dedup_keys = [k for k in [\"game_pk\",\"at_bat_number\",\"pitch_number\"] if k in df.columns]\n",
    "    if dedup_keys:\n",
    "        before = len(df)\n",
    "        df = df.drop_duplicates(subset=dedup_keys, keep=\"first\")\n",
    "        print(f\"Dedup pitches: {before} -> {len(df)}\")\n",
    "    else:\n",
    "        print(\"[WARN] Dedup keys not all present; skipped de-duplication.\")\n",
    "    return df\n",
    "\n",
    "def pick_spin_col(df):\n",
    "    if \"release_spin_rate\" in df.columns:\n",
    "        df[\"spin_rate\"] = df[\"release_spin_rate\"]\n",
    "    elif \"spin_rate\" in df.columns:\n",
    "        pass \n",
    "    else:\n",
    "        df[\"spin_rate\"] = np.nan\n",
    "    return df\n",
    "\n",
    "def ensure_injured(df):\n",
    "    \"\"\"If 'injured' column missing, try to create 2023 season-level label using injuries file.\n",
    "       We mark pitcher-season injured=1 for all pitches in 2023 if the pitcher appears in injuries_2023 file.\n",
    "       For 2021-2022 we leave as 0 if no data is available.\"\"\"\n",
    "    if \"injured\" in df.columns:\n",
    "        return df\n",
    "\n",
    "    if not p_inj.exists():\n",
    "        print(\"[WARN] injuries_2023.csv not found. Creating placeholder injured=0.\")\n",
    "        df[\"injured\"] = 0\n",
    "        return df\n",
    "\n",
    "    inj = pd.read_csv(p_inj, low_memory=False)\n",
    "    cols = {c.lower(): c for c in inj.columns}\n",
    "    id_candidates = [\"mlbam_id\",\"player_id_mlbam\",\"playerid\",\"mlb_id\",\"player_id\",\"mlbamid\",\"mlbam\"]\n",
    "    name_candidates = [\"player_name\",\"name\",\"Player\",\"PLAYER\",\"Name\"]\n",
    "\n",
    "    id_col = next((cols[c] for c in id_candidates if c in cols), None)\n",
    "    name_col = next((cols[c] for c in name_candidates if c in cols), None)\n",
    "\n",
    "    injured_ids = set()\n",
    "    injured_names = set()\n",
    "\n",
    "    if id_col is not None:\n",
    "        injured_ids = set(inj[id_col].dropna().astype(str).unique().tolist())\n",
    "    if name_col is not None:\n",
    "        injured_names = set(inj[name_col].dropna().astype(str).str.strip().unique().tolist())\n",
    "\n",
    "    if \"pitcher\" in df.columns:\n",
    "        df[\"_pitcher_str\"] = df[\"pitcher\"].astype(str)\n",
    "    else:\n",
    "        df[\"_pitcher_str\"] = \"\"\n",
    "\n",
    "    name_cols_df = [c for c in df.columns if c.lower() in [\"pitcher_name\",\"player_name\",\"name\"]]\n",
    "    if name_cols_df:\n",
    "        df[\"_pname\"] = df[name_cols_df[0]].astype(str).str.strip()\n",
    "    else:\n",
    "        df[\"_pname\"] = \"\"\n",
    "\n",
    "    df[\"injured\"] = 0\n",
    "    if \"game_year\" not in df.columns and \"game_date\" in df.columns:\n",
    "        df[\"game_year\"] = pd.to_datetime(df[\"game_date\"], errors=\"coerce\").dt.year\n",
    "\n",
    "    mask_2023 = (df[\"game_year\"] == 2023) if \"game_year\" in df.columns else pd.Series(False, index=df.index)\n",
    "    if len(injured_ids) > 0 and df[\"_pitcher_str\"].notna().any():\n",
    "        df.loc[mask_2023 & df[\"_pitcher_str\"].isin(injured_ids), \"injured\"] = 1\n",
    "    if len(injured_names) > 0 and df[\"_pname\"].notna().any():\n",
    "        df.loc[mask_2023 & df[\"_pname\"].isin(injured_names), \"injured\"] = 1\n",
    "\n",
    "    df = df.drop(columns=[c for c in [\"_pitcher_str\",\"_pname\"] if c in df.columns])\n",
    "    return df\n",
    "\n",
    "if p_merged.exists():\n",
    "    print(f\"Loading existing merged (root) CSV: {p_merged.name}\")\n",
    "    dfm = pd.read_csv(p_merged, low_memory=False)\n",
    "elif p_gz_in.exists():\n",
    "    print(f\"Loading existing merged (gz) CSV: {p_gz_in.name}\")\n",
    "    dfm = pd.read_csv(p_gz_in, low_memory=False, compression=\"gzip\")\n",
    "else:\n",
    "    print(\"No merged file found; loading statcast and constructing season-level injury label from injuries_2023.csv\")\n",
    "    dfm = load_statcast()\n",
    "    dfm = pick_spin_col(dfm)\n",
    "    dfm = ensure_injured(dfm)\n",
    "\n",
    "if \"game_date\" in dfm.columns:\n",
    "    dfm[\"game_date\"] = pd.to_datetime(dfm[\"game_date\"], errors=\"coerce\")\n",
    "if \"game_year\" not in dfm.columns and \"game_date\" in dfm.columns:\n",
    "    dfm[\"game_year\"] = dfm[\"game_date\"].dt.year\n",
    "dfm = pick_spin_col(dfm)\n",
    "\n",
    "dedup_keys = [k for k in [\"game_pk\",\"at_bat_number\",\"pitch_number\"] if k in dfm.columns]\n",
    "if dedup_keys:\n",
    "    before = len(dfm)\n",
    "    dfm = dfm.drop_duplicates(subset=dedup_keys, keep=\"first\")\n",
    "    print(f\"Final dedup: {before} -> {len(dfm)}\")\n",
    "else:\n",
    "    print(\"[WARN] Dedup keys not all present; skipped de-duplication.\")\n",
    "\n",
    "if \"game_year\" in dfm.columns:\n",
    "    print(\"\\nPitch counts by year (post-merge):\")\n",
    "    print(dfm[\"game_year\"].value_counts().sort_index())\n",
    "if \"injured\" in dfm.columns:\n",
    "    print(\"\\nPitch-level injury flag counts (post-merge):\")\n",
    "    print(dfm[\"injured\"].value_counts())\n",
    "\n",
    "dfm.to_csv(p_gz_out, index=False, compression=\"gzip\")\n",
    "print(f\"\\nSaved canonical merged to: {p_gz_out.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7065c476-0d82-4850-9cf4-b6e24f674d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "TARGET_FILE = \"mlb_pitch_data_2021_2023.csv\"\n",
    "\n",
    "def find_root_and_data():\n",
    "    cwd = Path.cwd().resolve()\n",
    "    print(\"CWD:\", cwd)\n",
    "\n",
    "    for p in [cwd, *cwd.parents]:\n",
    "        candidate = p / \"data\" / TARGET_FILE\n",
    "        if candidate.exists():\n",
    "            print(\"Found data file at:\", candidate)\n",
    "            return p, p / \"data\"\n",
    "\n",
    "    if cwd.name == \"data\" and (cwd / TARGET_FILE).exists():\n",
    "        print(\"Notebook appears to be in /data; using its parent as ROOT.\")\n",
    "        return cwd.parent, cwd\n",
    "\n",
    "    for base in [cwd, cwd.parent]:\n",
    "        try:\n",
    "            hit = next(base.rglob(TARGET_FILE))\n",
    "            print(\"Found by scanning:\", hit)\n",
    "            root = hit.parent.parent if hit.parent.name == \"data\" else hit.parents[2]\n",
    "            return root, root / \"data\"\n",
    "        except StopIteration:\n",
    "            pass\n",
    "\n",
    "    raise FileNotFoundError(f\"Could not locate {TARGET_FILE} by walking parents or light scan.\")\n",
    "\n",
    "ROOT, DATA = find_root_and_data()\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"DATA:\", DATA)\n",
    "\n",
    "try:\n",
    "    listing = sorted(DATA.iterdir())[:10]\n",
    "    print(\"\\n/data contents (first 10):\")\n",
    "    for x in listing:\n",
    "        print(\" -\", x.name)\n",
    "except Exception as e:\n",
    "    print(\"Could not list /data:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32f70b4-abfd-4be7-92f3-d372f6369004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    ROOT, DATA\n",
    "except NameError:\n",
    "    from pathlib import Path\n",
    "    TARGET_FILE = \"mlb_pitch_data_2021_2023.csv\"\n",
    "    def _find():\n",
    "        cwd = Path.cwd().resolve()\n",
    "        for p in [cwd, *cwd.parents]:\n",
    "            if (p / \"data\" / TARGET_FILE).exists():\n",
    "                return p, p / \"data\"\n",
    "        if cwd.name == \"data\" and (cwd / TARGET_FILE).exists():\n",
    "            return cwd.parent, cwd\n",
    "        raise FileNotFoundError(\"Cannot auto-detect project root.\")\n",
    "    ROOT, DATA = _find()\n",
    "\n",
    "DATA.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "p_main   = DATA / \"mlb_pitch_data_2021_2023.csv\"\n",
    "p_merged = ROOT / \"pitches_with_injuries_2021_2023.csv\"        \n",
    "p_gz_out = DATA / \"pitches_with_injuries_2021_2023.csv.gz\"\n",
    "p_gz_in  = DATA / \"pitches_with_injuries_2021_2023.csv.gz\"\n",
    "p_inj    = DATA / \"rosterresource_injuries_2023.csv\"\n",
    "\n",
    "print(\"Using:\")\n",
    "print(\"  ROOT:\", ROOT)\n",
    "print(\"  DATA:\", DATA)\n",
    "print(\"  main:\", p_main.exists(), p_main)\n",
    "print(\"merged:\", p_merged.exists(), p_merged)\n",
    "print(\" merged.gz exists:\", p_gz_in.exists(), p_gz_in)\n",
    "print(\"injuries:\", p_inj.exists(), p_inj)\n",
    "\n",
    "def load_statcast():\n",
    "    if not p_main.exists():\n",
    "        raise FileNotFoundError(f\"Expected Statcast at {p_main}\")\n",
    "    df = pd.read_csv(p_main, low_memory=False)\n",
    "    if \"game_date\" in df.columns:\n",
    "        df[\"game_date\"] = pd.to_datetime(df[\"game_date\"], errors=\"coerce\")\n",
    "    if \"game_year\" not in df.columns and \"game_date\" in df.columns:\n",
    "        df[\"game_year\"] = df[\"game_date\"].dt.year\n",
    "    dedup_keys = [k for k in [\"game_pk\",\"at_bat_number\",\"pitch_number\"] if k in df.columns]\n",
    "    if dedup_keys:\n",
    "        before = len(df)\n",
    "        df = df.drop_duplicates(subset=dedup_keys, keep=\"first\")\n",
    "        print(f\"Dedup pitches: {before} -> {len(df)}\")\n",
    "    else:\n",
    "        print(\"[WARN] Dedup keys not all present; skipped de-duplication.\")\n",
    "    return df\n",
    "\n",
    "def pick_spin_col(df):\n",
    "    if \"release_spin_rate\" in df.columns and \"spin_rate\" not in df.columns:\n",
    "        df[\"spin_rate\"] = df[\"release_spin_rate\"]\n",
    "    elif \"spin_rate\" not in df.columns:\n",
    "        df[\"spin_rate\"] = np.nan\n",
    "    return df\n",
    "\n",
    "def ensure_injured(df):\n",
    "    \"\"\"If 'injured' column missing, try to create 2023 season-level label using injuries file.\n",
    "       Mark pitcher-season injured=1 for all 2023 pitches if pitcher appears in injuries_2023 file.\"\"\"\n",
    "    if \"injured\" in df.columns:\n",
    "        return df\n",
    "\n",
    "    if not p_inj.exists():\n",
    "        print(\"[WARN] injuries_2023.csv not found. Creating placeholder injured=0.\")\n",
    "        df[\"injured\"] = 0\n",
    "        return df\n",
    "\n",
    "    inj = pd.read_csv(p_inj, low_memory=False)\n",
    "    cols = {c.lower(): c for c in inj.columns}\n",
    "    id_candidates = [\"mlbam_id\",\"player_id_mlbam\",\"playerid\",\"mlb_id\",\"player_id\",\"mlbamid\",\"mlbam\"]\n",
    "    name_candidates = [\"player_name\",\"name\",\"player\",\"Name\",\"PLAYER\"]\n",
    "\n",
    "    id_col = next((cols[c] for c in id_candidates if c in cols), None)\n",
    "    name_col = next((cols[c] for c in name_candidates if c in cols), None)\n",
    "\n",
    "    injured_ids = set(inj[id_col].dropna().astype(str)) if id_col else set()\n",
    "    injured_names = set(inj[name_col].dropna().astype(str).str.strip()) if name_col else set()\n",
    "\n",
    "    if \"game_year\" not in df.columns and \"game_date\" in df.columns:\n",
    "        df[\"game_year\"] = pd.to_datetime(df[\"game_date\"], errors=\"coerce\").dt.year\n",
    "\n",
    "    df[\"_pitcher_str\"] = df.get(\"pitcher\", pd.Series(index=df.index, dtype=\"float\")).astype(str)\n",
    "    name_cols_df = [c for c in df.columns if c.lower() in [\"pitcher_name\",\"player_name\",\"name\"]]\n",
    "    df[\"_pname\"] = df[name_cols_df[0]].astype(str).str.strip() if name_cols_df else \"\"\n",
    "\n",
    "    df[\"injured\"] = 0\n",
    "    mask_2023 = (df[\"game_year\"] == 2023) if \"game_year\" in df.columns else pd.Series(False, index=df.index)\n",
    "\n",
    "    if injured_ids:\n",
    "        df.loc[mask_2023 & df[\"_pitcher_str\"].isin(injured_ids), \"injured\"] = 1\n",
    "    if injured_names:\n",
    "        df.loc[mask_2023 & df[\"_pname\"].isin(injured_names), \"injured\"] = 1\n",
    "\n",
    "    return df.drop(columns=[c for c in [\"_pitcher_str\",\"_pname\"] if c in df.columns])\n",
    "\n",
    "if p_merged.exists():\n",
    "    print(f\"\\nLoading existing merged (root) CSV: {p_merged.name}\")\n",
    "    dfm = pd.read_csv(p_merged, low_memory=False)\n",
    "elif p_gz_in.exists():\n",
    "    print(f\"\\nLoading existing merged (gz) CSV: {p_gz_in.name}\")\n",
    "    dfm = pd.read_csv(p_gz_in, low_memory=False, compression=\"gzip\")\n",
    "else:\n",
    "    print(\"\\nNo merged file found; loading statcast and constructing 2023 season-level injury label from injuries_2023.csv\")\n",
    "    dfm = load_statcast()\n",
    "    dfm = pick_spin_col(dfm)\n",
    "    dfm = ensure_injured(dfm)\n",
    "\n",
    "if \"game_date\" in dfm.columns:\n",
    "    dfm[\"game_date\"] = pd.to_datetime(dfm[\"game_date\"], errors=\"coerce\")\n",
    "if \"game_year\" not in dfm.columns and \"game_date\" in dfm.columns:\n",
    "    dfm[\"game_year\"] = dfm[\"game_date\"].dt.year\n",
    "dfm = pick_spin_col(dfm)\n",
    "\n",
    "dedup_keys = [k for k in [\"game_pk\",\"at_bat_number\",\"pitch_number\"] if k in dfm.columns]\n",
    "if dedup_keys:\n",
    "    before = len(dfm)\n",
    "    dfm = dfm.drop_duplicates(subset=dedup_keys, keep=\"first\")\n",
    "    print(f\"Final dedup: {before} -> {len(dfm)}\")\n",
    "else:\n",
    "    print(\"[WARN] Dedup keys not all present; skipped de-duplication.\")\n",
    "\n",
    "if \"game_year\" in dfm.columns:\n",
    "    print(\"\\nPitch counts by year (post-merge):\")\n",
    "    print(dfm[\"game_year\"].value_counts().sort_index())\n",
    "if \"injured\" in dfm.columns:\n",
    "    print(\"\\nPitch-level injury flag counts (post-merge):\")\n",
    "    print(dfm[\"injured\"].value_counts())\n",
    "\n",
    "dfm.to_csv(p_gz_out, index=False, compression=\"gzip\")\n",
    "print(f\"\\nSaved canonical merged to: {p_gz_out.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb99e2c-a7fc-48c7-8f67-e5b5725dc1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd() if (Path.cwd()/ \"data\").exists() else (Path.cwd()/ \"pitcher-injury-predictor\")\n",
    "DATA = ROOT / \"data\"\n",
    "p_gz = DATA / \"pitches_with_injuries_2021_2023.csv.gz\"\n",
    "assert p_gz.exists(), f\"Missing {p_gz}\"\n",
    "\n",
    "df = pd.read_csv(p_gz, low_memory=False, compression=\"gzip\")\n",
    "if \"game_date\" in df.columns:\n",
    "    df[\"game_date\"] = pd.to_datetime(df[\"game_date\"], errors=\"coerce\")\n",
    "if \"game_year\" not in df.columns and \"game_date\" in df.columns:\n",
    "    df[\"game_year\"] = df[\"game_date\"].dt.year\n",
    "\n",
    "dedup_keys = [k for k in [\"game_pk\",\"at_bat_number\",\"pitch_number\"] if k in df.columns]\n",
    "if dedup_keys:\n",
    "    df = df.drop_duplicates(subset=dedup_keys, keep=\"first\")\n",
    "\n",
    "df23 = df[df[\"game_year\"] == 2023].copy()\n",
    "if df23.empty:\n",
    "    raise ValueError(\"No 2023 data found after filtering.\")\n",
    "\n",
    "if \"release_spin_rate\" in df23.columns and \"spin_rate\" not in df23.columns:\n",
    "    df23[\"spin_rate\"] = df23[\"release_spin_rate\"]\n",
    "elif \"spin_rate\" not in df23.columns:\n",
    "    df23[\"spin_rate\"] = np.nan\n",
    "\n",
    "for col in [\"pitcher\",\"pitcher_name\",\"game_pk\",\"game_date\",\"pitch_type\",\"release_speed\",\"spin_rate\"]:\n",
    "    if col not in df23.columns:\n",
    "        df23[col] = np.nan\n",
    "\n",
    "gcols = [\"pitcher\",\"pitcher_name\",\"game_pk\",\"game_date\"]\n",
    "per_game = (df23\n",
    "            .dropna(subset=[\"pitcher\"])\n",
    "            .groupby(gcols, dropna=False)\n",
    "            .size()\n",
    "            .reset_index(name=\"pitches_in_game\"))\n",
    "\n",
    "per_game = per_game.sort_values([\"pitcher\",\"game_date\"])\n",
    "per_game[\"rest_days\"] = (per_game.groupby(\"pitcher\")[\"game_date\"]\n",
    "                         .diff().dt.days)\n",
    "\n",
    "rest_agg = (per_game.groupby([\"pitcher\",\"pitcher_name\"], dropna=False)\n",
    "            .agg(\n",
    "                games_pitched=(\"game_pk\",\"nunique\"),\n",
    "                total_pitches=(\"pitches_in_game\",\"sum\"),\n",
    "                avg_pitches_per_game=(\"pitches_in_game\",\"mean\"),\n",
    "                max_pitches_in_game=(\"pitches_in_game\",\"max\"),\n",
    "                median_rest_days=(\"rest_days\",\"median\"),\n",
    "                mean_rest_days=(\"rest_days\",\"mean\"),\n",
    "                short_rest_games=(\"rest_days\", lambda s: np.sum(s<=3) if s.notna().any() else 0),\n",
    "            )\n",
    "            .reset_index())\n",
    "\n",
    "def pct95(x): \n",
    "    try: \n",
    "        return np.nanpercentile(x, 95) \n",
    "    except Exception: \n",
    "        return np.nan\n",
    "\n",
    "velo_spin = (df23.groupby([\"pitcher\",\"pitcher_name\"], dropna=False)\n",
    "             .agg(\n",
    "                 avg_velocity=(\"release_speed\",\"mean\"),\n",
    "                 p95_velocity=(\"release_speed\", pct95),\n",
    "                 avg_spin=(\"spin_rate\",\"mean\"),\n",
    "             )\n",
    "             .reset_index())\n",
    "\n",
    "fastballs = {\"FF\",\"FA\",\"FT\",\"SI\",\"FC\"}\n",
    "breaking  = {\"SL\",\"CU\",\"KC\",\"SV\",\"SC\"}\n",
    "offspeed  = {\"CH\",\"FS\",\"KN\"}\n",
    "\n",
    "tmp = df23.copy()\n",
    "tmp[\"pitch_type\"] = tmp[\"pitch_type\"].astype(str).str.upper()\n",
    "tmp[\"family\"] = np.where(tmp[\"pitch_type\"].isin(fastballs), \"fastball\",\n",
    "                 np.where(tmp[\"pitch_type\"].isin(breaking),  \"breaking\",\n",
    "                 np.where(tmp[\"pitch_type\"].isin(offspeed),  \"offspeed\",\"other\")))\n",
    "mix = (tmp.groupby([\"pitcher\",\"pitcher_name\",\"family\"], dropna=False)\n",
    "       .size().reset_index(name=\"cnt\"))\n",
    "mix_tot = mix.groupby([\"pitcher\",\"pitcher_name\"], dropna=False)[\"cnt\"].sum().rename(\"total\").reset_index()\n",
    "mix = mix.merge(mix_tot, on=[\"pitcher\",\"pitcher_name\"], how=\"left\")\n",
    "mix[\"pct\"] = mix[\"cnt\"] / mix[\"total\"].replace(0, np.nan)\n",
    "\n",
    "mix_pivot = (mix.pivot_table(index=[\"pitcher\",\"pitcher_name\"], columns=\"family\", values=\"pct\", fill_value=0)\n",
    "               .reset_index())\n",
    "mix_pivot.columns = [\"pitcher\",\"pitcher_name\"] + [f\"mix_{c}\" for c in mix_pivot.columns.tolist()[2:]]\n",
    "\n",
    "if \"injured\" not in df23.columns:\n",
    "    df23[\"injured\"] = 0\n",
    "label_agg = (df23.groupby([\"pitcher\",\"pitcher_name\"], dropna=False)[\"injured\"]\n",
    "             .max().reset_index().rename(columns={\"injured\":\"injury_2023\"}))\n",
    "\n",
    "feat = (rest_agg\n",
    "        .merge(velo_spin, on=[\"pitcher\",\"pitcher_name\"], how=\"outer\")\n",
    "        .merge(mix_pivot, on=[\"pitcher\",\"pitcher_name\"], how=\"outer\")\n",
    "        .merge(label_agg, on=[\"pitcher\",\"pitcher_name\"], how=\"left\"))\n",
    "\n",
    "for c in [\"mix_fastball\",\"mix_breaking\",\"mix_offspeed\",\"mix_other\"]:\n",
    "    if c not in feat.columns:\n",
    "        feat[c] = 0.0\n",
    "num_cols = feat.select_dtypes(include=[np.number]).columns\n",
    "feat[num_cols] = feat[num_cols].replace([np.inf,-np.inf], np.nan)\n",
    "\n",
    "p_feat = DATA / \"features_2023.csv\"\n",
    "feat.to_csv(p_feat, index=False)\n",
    "print(f\"Saved 2023 feature table: {p_feat.resolve()}\")\n",
    "print(\"Rows (pitchers):\", len(feat))\n",
    "print(\"\\nLabel distribution (injury_2023):\")\n",
    "print(feat[\"injury_2023\"].fillna(0).astype(int).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b659ef7-0df2-4e3a-97b0-558ff1612e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "MERGED_GZ_NAME = \"pitches_with_injuries_2021_2023.csv.gz\"\n",
    "MERGED_CSV_NAME = \"pitches_with_injuries_2021_2023.csv\"\n",
    "MAIN_PITCHES = \"mlb_pitch_data_2021_2023.csv\"\n",
    "\n",
    "def find_root_and_data():\n",
    "    cwd = Path.cwd().resolve()\n",
    "\n",
    "    if cwd.name == \"data\":\n",
    "        root = cwd.parent\n",
    "        data = cwd\n",
    "        return root, data\n",
    "\n",
    "    if (cwd / \"data\").exists():\n",
    "        return cwd, cwd / \"data\"\n",
    "\n",
    "    for p in cwd.parents:\n",
    "        if (p / \"data\").exists():\n",
    "            return p, p / \"data\"\n",
    "\n",
    "    return cwd, cwd / \"data\"\n",
    "\n",
    "def find_file(name):\n",
    "    \"\"\"Find file by walking up parents and doing a short downward scan.\"\"\"\n",
    "    root, data = find_root_and_data()\n",
    "    p1 = data / name\n",
    "    if p1.exists():\n",
    "        return root, data, p1\n",
    "    p2 = root / name\n",
    "    if p2.exists():\n",
    "        return root, data, p2\n",
    "    try:\n",
    "        hit = next((p for p in root.rglob(name)))\n",
    "        if hit.parent.name == \"data\":\n",
    "            root2 = hit.parent.parent\n",
    "            data2 = hit.parent\n",
    "            return root2, data2, hit\n",
    "        return root, data, hit\n",
    "    except StopIteration:\n",
    "        return root, data, None\n",
    "\n",
    "ROOT, DATA, p_gz = find_file(MERGED_GZ_NAME)\n",
    "_, _, p_csv = find_file(MERGED_CSV_NAME)\n",
    "\n",
    "if p_gz is None and p_csv is None:\n",
    "    _, _, p_main = find_file(MAIN_PITCHES)\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not find {MERGED_GZ_NAME} or {MERGED_CSV_NAME}.\\n\"\n",
    "        f\"Check that step (B) saved the merged file.\\n\"\n",
    "        f\"Detected ROOT={ROOT}, DATA={DATA}. \"\n",
    "        f\"Main pitches present? {bool(p_main)} at {p_main}\"\n",
    "    )\n",
    "\n",
    "DATA.mkdir(parents=True, exist_ok=True)\n",
    "if p_gz is None and p_csv is not None:\n",
    "    print(f\"Found merged CSV at {p_csv}. Compressing to {DATA / MERGED_GZ_NAME} ...\")\n",
    "    df_tmp = pd.read_csv(p_csv, low_memory=False)\n",
    "    (DATA / MERGED_GZ_NAME).write_text(\"\") \n",
    "    df_tmp.to_csv(DATA / MERGED_GZ_NAME, index=False, compression=\"gzip\")\n",
    "    p_gz = DATA / MERGED_GZ_NAME\n",
    "    print(\"Compressed.\")\n",
    "\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"DATA:\", DATA)\n",
    "print(\"Using merged file:\", p_gz)\n",
    "\n",
    "df = pd.read_csv(p_gz, low_memory=False, compression=\"gzip\")\n",
    "\n",
    "if \"game_date\" in df.columns:\n",
    "    df[\"game_date\"] = pd.to_datetime(df[\"game_date\"], errors=\"coerce\")\n",
    "if \"game_year\" not in df.columns and \"game_date\" in df.columns:\n",
    "    df[\"game_year\"] = df[\"game_date\"].dt.year\n",
    "\n",
    "dedup_keys = [k for k in [\"game_pk\",\"at_bat_number\",\"pitch_number\"] if k in df.columns]\n",
    "if dedup_keys:\n",
    "    before = len(df)\n",
    "    df = df.drop_duplicates(subset=dedup_keys, keep=\"first\")\n",
    "    print(f\"Dedup pitches: {before} -> {len(df)}\")\n",
    "\n",
    "df23 = df[df[\"game_year\"] == 2023].copy()\n",
    "if df23.empty:\n",
    "    raise ValueError(\"No 2023 data found after filtering.\")\n",
    "\n",
    "if \"release_spin_rate\" in df23.columns and \"spin_rate\" not in df23.columns:\n",
    "    df23[\"spin_rate\"] = df23[\"release_spin_rate\"]\n",
    "elif \"spin_rate\" not in df23.columns:\n",
    "    df23[\"spin_rate\"] = np.nan\n",
    "\n",
    "for col in [\"pitcher\",\"pitcher_name\",\"game_pk\",\"game_date\",\"pitch_type\",\"release_speed\",\"spin_rate\",\"injured\"]:\n",
    "    if col not in df23.columns:\n",
    "        df23[col] = np.nan if col != \"injured\" else 0\n",
    "\n",
    "gcols = [\"pitcher\",\"pitcher_name\",\"game_pk\",\"game_date\"]\n",
    "per_game = (df23\n",
    "            .dropna(subset=[\"pitcher\"])\n",
    "            .groupby(gcols, dropna=False)\n",
    "            .size()\n",
    "            .reset_index(name=\"pitches_in_game\"))\n",
    "per_game = per_game.sort_values([\"pitcher\",\"game_date\"])\n",
    "per_game[\"rest_days\"] = (per_game.groupby(\"pitcher\")[\"game_date\"]\n",
    "                         .diff().dt.days)\n",
    "\n",
    "rest_agg = (per_game.groupby([\"pitcher\",\"pitcher_name\"], dropna=False)\n",
    "            .agg(\n",
    "                games_pitched=(\"game_pk\",\"nunique\"),\n",
    "                total_pitches=(\"pitches_in_game\",\"sum\"),\n",
    "                avg_pitches_per_game=(\"pitches_in_game\",\"mean\"),\n",
    "                max_pitches_in_game=(\"pitches_in_game\",\"max\"),\n",
    "                median_rest_days=(\"rest_days\",\"median\"),\n",
    "                mean_rest_days=(\"rest_days\",\"mean\"),\n",
    "                short_rest_games=(\"rest_days\", lambda s: np.sum(s<=3) if s.notna().any() else 0),\n",
    "            )\n",
    "            .reset_index())\n",
    "\n",
    "def pct95(x): \n",
    "    try: \n",
    "        return np.nanpercentile(x, 95) \n",
    "    except Exception: \n",
    "        return np.nan\n",
    "\n",
    "velo_spin = (df23.groupby([\"pitcher\",\"pitcher_name\"], dropna=False)\n",
    "             .agg(\n",
    "                 avg_velocity=(\"release_speed\",\"mean\"),\n",
    "                 p95_velocity=(\"release_speed\", pct95),\n",
    "                 avg_spin=(\"spin_rate\",\"mean\"),\n",
    "             )\n",
    "             .reset_index())\n",
    "\n",
    "fastballs = {\"FF\",\"FA\",\"FT\",\"SI\",\"FC\"}\n",
    "breaking  = {\"SL\",\"CU\",\"KC\",\"SV\",\"SC\"}\n",
    "offspeed  = {\"CH\",\"FS\",\"KN\"}\n",
    "\n",
    "tmp = df23.copy()\n",
    "tmp[\"pitch_type\"] = tmp[\"pitch_type\"].astype(str).str.upper()\n",
    "tmp[\"family\"] = np.where(tmp[\"pitch_type\"].isin(fastballs), \"fastball\",\n",
    "                 np.where(tmp[\"pitch_type\"].isin(breaking),  \"breaking\",\n",
    "                 np.where(tmp[\"pitch_type\"].isin(offspeed),  \"offspeed\",\"other\")))\n",
    "mix = (tmp.groupby([\"pitcher\",\"pitcher_name\",\"family\"], dropna=False)\n",
    "       .size().reset_index(name=\"cnt\"))\n",
    "mix_tot = mix.groupby([\"pitcher\",\"pitcher_name\"], dropna=False)[\"cnt\"].sum().rename(\"total\").reset_index()\n",
    "mix = mix.merge(mix_tot, on=[\"pitcher\",\"pitcher_name\"], how=\"left\")\n",
    "mix[\"pct\"] = mix[\"cnt\"] / mix[\"total\"].replace(0, np.nan)\n",
    "mix_pivot = (mix.pivot_table(index=[\"pitcher\",\"pitcher_name\"], columns=\"family\", values=\"pct\", fill_value=0)\n",
    "               .reset_index())\n",
    "mix_pivot.columns = [\"pitcher\",\"pitcher_name\"] + [f\"mix_{c}\" for c in mix_pivot.columns.tolist()[2:]]\n",
    "\n",
    "label_agg = (df23.groupby([\"pitcher\",\"pitcher_name\"], dropna=False)[\"injured\"]\n",
    "             .max().reset_index().rename(columns={\"injured\":\"injury_2023\"}))\n",
    "\n",
    "feat = (rest_agg\n",
    "        .merge(velo_spin, on=[\"pitcher\",\"pitcher_name\"], how=\"outer\")\n",
    "        .merge(mix_pivot, on=[\"pitcher\",\"pitcher_name\"], how=\"outer\")\n",
    "        .merge(label_agg, on=[\"pitcher\",\"pitcher_name\"], how=\"left\"))\n",
    "\n",
    "for c in [\"mix_fastball\",\"mix_breaking\",\"mix_offspeed\",\"mix_other\"]:\n",
    "    if c not in feat.columns:\n",
    "        feat[c] = 0.0\n",
    "\n",
    "num_cols = feat.select_dtypes(include=[np.number]).columns\n",
    "feat[num_cols] = feat[num_cols].replace([np.inf,-np.inf], np.nan)\n",
    "\n",
    "p_feat = (DATA / \"features_2023.csv\")\n",
    "feat.to_csv(p_feat, index=False)\n",
    "print(f\"Saved 2023 feature table: {p_feat.resolve()}\")\n",
    "print(\"Rows (pitchers):\", len(feat))\n",
    "print(\"\\nLabel distribution (injury_2023):\")\n",
    "print(feat[\"injury_2023\"].fillna(0).astype(int).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810fb639-b7e8-4025-8862-8e2cf9782bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "FEATURES_NAME = \"features_2023.csv\"\n",
    "\n",
    "def find_root_and_data():\n",
    "    cwd = Path.cwd().resolve()\n",
    "    if cwd.name == \"data\":\n",
    "        return cwd.parent, cwd\n",
    "    if (cwd / \"data\").exists():\n",
    "        return cwd, cwd / \"data\"\n",
    "    for p in cwd.parents:\n",
    "        if (p / \"data\").exists():\n",
    "            return p, p / \"data\"\n",
    "    return cwd, cwd / \"data\"\n",
    "\n",
    "def find_file(name):\n",
    "    root, data = find_root_and_data()\n",
    "    p1 = data / name\n",
    "    if p1.exists():\n",
    "        return root, data, p1\n",
    "    p2 = root / name\n",
    "    if p2.exists():\n",
    "        return root, data, p2\n",
    "    try:\n",
    "        hit = next(root.rglob(name))\n",
    "        if hit.parent.name == \"data\":\n",
    "            return hit.parent.parent, hit.parent, hit\n",
    "        return root, data, hit\n",
    "    except StopIteration:\n",
    "        return root, data, None\n",
    "\n",
    "ROOT, DATA, p_feat = find_file(FEATURES_NAME)\n",
    "assert p_feat is not None and p_feat.exists(), f\"Missing {FEATURES_NAME}; run step (C). Found ROOT={ROOT}, DATA={DATA}\"\n",
    "\n",
    "print(\"Using features file:\", p_feat)\n",
    "\n",
    "df = pd.read_csv(p_feat)\n",
    "\n",
    "y = df.get(\"injury_2023\", pd.Series(0, index=df.index)).fillna(0).astype(int)\n",
    "drop_cols = [\"injury_2023\",\"pitcher\",\"pitcher_name\"]\n",
    "X = df.drop(columns=[c for c in drop_cols if c in df.columns], errors=\"ignore\")\n",
    "\n",
    "X = X.select_dtypes(include=[np.number]).copy()\n",
    "X = X.loc[:, X.notna().any(axis=0)]\n",
    "X = X.fillna(X.median(numeric_only=True))\n",
    "\n",
    "print(\"X shape:\", X.shape, \"| y positives:\", int(y.sum()), \"/\", len(y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "lr_pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"lr\", LogisticRegression(max_iter=1000, class_weight=\"balanced\", solver=\"lbfgs\"))\n",
    "])\n",
    "lr_pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr_pipe.predict(X_test)\n",
    "y_proba = lr_pipe.predict_proba(X_test)[:, 1]\n",
    "auc_lr = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "print(\"\\n=== Logistic Regression: Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "print(f\"ROC AUC (LR): {auc_lr:.3f}\")\n",
    "\n",
    "baseline_lr_report = classification_report(y_test, y_pred, digits=3, output_dict=True)\n",
    "baseline_lr_auc = float(auc_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5f8852-696d-4d3c-84ab-9ee5237fdbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def find_root_and_data():\n",
    "    cwd = Path.cwd().resolve()\n",
    "    if cwd.name == \"data\":\n",
    "        return cwd.parent, cwd\n",
    "    if (cwd / \"data\").exists():\n",
    "        return cwd, cwd / \"data\"\n",
    "    for p in cwd.parents:\n",
    "        if (p / \"data\").exists():\n",
    "            return p, p / \"data\"\n",
    "    return cwd, cwd / \"data\"\n",
    "\n",
    "ROOT, DATA = find_root_and_data()\n",
    "p_feat = DATA / \"features_2023.csv\"\n",
    "p_gz   = DATA / \"pitches_with_injuries_2021_2023.csv.gz\"\n",
    "p_inj  = DATA / \"rosterresource_injuries_2023.csv\"\n",
    "\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"DATA:\", DATA)\n",
    "print(\"Exists:\", {\"features\": p_feat.exists(), \"merged_gz\": p_gz.exists(), \"injuries\": p_inj.exists()})\n",
    "\n",
    "df_feat = pd.read_csv(p_feat) if p_feat.exists() else None\n",
    "if df_feat is not None:\n",
    "    vc = df_feat[\"injury_2023\"].fillna(0).astype(int).value_counts(dropna=False)\n",
    "    print(\"\\nfeatures_2023 label counts (injury_2023):\")\n",
    "    print(vc)\n",
    "\n",
    "if p_inj.exists():\n",
    "    inj = pd.read_csv(p_inj, low_memory=False)\n",
    "    print(\"\\nInjuries columns:\", list(inj.columns)[:20])\n",
    "    print(\"Injuries rows:\", len(inj))\n",
    "else:\n",
    "    inj = None\n",
    "\n",
    "if p_gz.exists():\n",
    "    dfm = pd.read_csv(p_gz, nrows=200_000, low_memory=False, compression=\"gzip\")  \n",
    "    print(\"\\nMerged sample columns:\", list(dfm.columns)[:25])\n",
    "    print(\"Has 'pitcher' ID column?\", \"pitcher\" in dfm.columns)\n",
    "    print(\"Has 'pitcher_name'?\", \"pitcher_name\" in dfm.columns)\n",
    "    if \"pitcher_name\" in dfm.columns:\n",
    "        names = dfm[\"pitcher_name\"].dropna().astype(str).unique()[:10]\n",
    "        print(\"Sample pitcher_name values:\", names)\n",
    "else:\n",
    "    dfm = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c76aebb-5af0-4768-a82f-e1f3abb94f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def normalize_name(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = s.strip()\n",
    "    s = \"\".join(c for c in unicodedata.normalize(\"NFKD\", s) if not unicodedata.combining(c))\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"\\b(jr|sr|ii|iii|iv|v)\\b\\.?\", \"\", s)\n",
    "    s = re.sub(r\"[^a-z\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "ROOT, DATA = find_root_and_data()\n",
    "p_feat = DATA / \"features_2023.csv\"\n",
    "p_gz   = DATA / \"pitches_with_injuries_2021_2023.csv.gz\"\n",
    "p_inj  = DATA / \"rosterresource_injuries_2023.csv\"\n",
    "\n",
    "assert p_feat.exists(), f\"Missing {p_feat}\"\n",
    "assert p_gz.exists(),   f\"Missing {p_gz}\"\n",
    "assert p_inj.exists(),  f\"Missing {p_inj}\"\n",
    "\n",
    "feat = pd.read_csv(p_feat)\n",
    "dfm  = pd.read_csv(p_gz, low_memory=False, compression=\"gzip\")\n",
    "inj  = pd.read_csv(p_inj, low_memory=False)\n",
    "\n",
    "inj_cols = {c.lower(): c for c in inj.columns}\n",
    "id_candidates   = [\"mlbam_id\",\"player_id_mlbam\",\"playerid\",\"mlb_id\",\"player_id\",\"mlbamid\",\"mlbam\"]\n",
    "name_candidates = [\"player_name\",\"name\",\"player\",\"Player\",\"PLAYER\",\"Name\"]\n",
    "inj_id_col   = next((inj_cols[c] for c in id_candidates if c in inj_cols), None)\n",
    "inj_name_col = next((inj_cols[c] for c in name_candidates if c in inj_cols), None)\n",
    "\n",
    "injured_ids = set()\n",
    "injured_names_norm = set()\n",
    "if inj_id_col is not None:\n",
    "    injured_ids = set(inj[inj_id_col].dropna().astype(str).tolist())\n",
    "if inj_name_col is not None:\n",
    "    injured_names_norm = set(inj[inj_name_col].dropna().astype(str).map(normalize_name).tolist())\n",
    "\n",
    "dfm[\"game_date\"] = pd.to_datetime(dfm.get(\"game_date\"), errors=\"coerce\")\n",
    "if \"game_year\" not in dfm.columns:\n",
    "    dfm[\"game_year\"] = dfm[\"game_date\"].dt.year\n",
    "df23 = dfm[dfm[\"game_year\"] == 2023].copy()\n",
    "\n",
    "pid_has = \"pitcher\" in df23.columns\n",
    "pname_has = \"pitcher_name\" in df23.columns\n",
    "\n",
    "if pid_has:\n",
    "    df23[\"_pid_str\"] = df23[\"pitcher\"].astype(str)\n",
    "else:\n",
    "    df23[\"_pid_str\"] = \"\"\n",
    "\n",
    "if pname_has:\n",
    "    df23[\"_pname_norm\"] = df23[\"pitcher_name\"].astype(str).map(normalize_name)\n",
    "else:\n",
    "    df23[\"_pname_norm\"] = \"\"\n",
    "\n",
    "df23[\"injured_tmp\"] = 0\n",
    "if injured_ids and pid_has:\n",
    "    df23.loc[df23[\"_pid_str\"].isin(injured_ids), \"injured_tmp\"] = 1\n",
    "if injured_names_norm and pname_has:\n",
    "    df23.loc[df23[\"_pname_norm\"].isin(injured_names_norm), \"injured_tmp\"] = 1\n",
    "\n",
    "lab = (df23.groupby(\"pitcher\", dropna=False)[\"injured_tmp\"]\n",
    "       .max().reset_index().rename(columns={\"injured_tmp\":\"injury_2023\"}))\n",
    "\n",
    "feat2 = feat.copy()\n",
    "if \"pitcher\" in feat2.columns:\n",
    "    feat2 = feat2.merge(lab, on=\"pitcher\", how=\"left\", suffixes=(\"\", \"_new\"))\n",
    "    if \"injury_2023_new\" in feat2.columns:\n",
    "        feat2[\"injury_2023\"] = feat2[\"injury_2023\"].fillna(feat2[\"injury_2023_new\"])\n",
    "        feat2 = feat2.drop(columns=[\"injury_2023_new\"])\n",
    "\n",
    "if feat2[\"injury_2023\"].isna().all() or (feat2[\"injury_2023\"].fillna(0).sum() == 0):\n",
    "    if \"pitcher_name\" in feat2.columns and pname_has:\n",
    "        key_feat = feat2[[\"pitcher_name\"]].copy()\n",
    "        key_feat[\"_pname_norm\"] = key_feat[\"pitcher_name\"].astype(str).map(normalize_name)\n",
    "        lab_name = (df23.groupby(\"_pname_norm\")[\"injured_tmp\"]\n",
    "                    .max().reset_index().rename(columns={\"injured_tmp\":\"injury_2023_name\"}))\n",
    "        feat2 = feat2.join(key_feat[\"_pname_norm\"])\n",
    "        feat2 = feat2.merge(lab_name, on=\"_pname_norm\", how=\"left\")\n",
    "        feat2[\"injury_2023\"] = feat2[\"injury_2023\"].fillna(feat2[\"injury_2023_name\"])\n",
    "        feat2 = feat2.drop(columns=[c for c in [\"_pname_norm\",\"injury_2023_name\"] if c in feat2.columns], errors=\"ignore\")\n",
    "\n",
    "feat2[\"injury_2023\"] = feat2[\"injury_2023\"].fillna(0).astype(int)\n",
    "\n",
    "print(\"Label counts after repair:\")\n",
    "print(feat2[\"injury_2023\"].value_counts())\n",
    "\n",
    "feat2.to_csv(p_feat, index=False)\n",
    "print(\"Updated labels saved to:\", p_feat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23e6fab-891b-475f-a5a7-816ca599d098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unicodedata, re\n",
    "from pathlib import Path\n",
    "\n",
    "DATA = Path.cwd() if Path.cwd().name == \"data\" else Path.cwd()/\"data\"\n",
    "\n",
    "injury_files = {\n",
    "    2021: DATA / \"2021roster-resource__injury-report.xlsx.xlsx\",\n",
    "    2022: DATA / \"2022roster-resource__injury-report.xlsx.xlsx\",\n",
    "    2023: DATA / \"2023roster-resource__injury-report.xlsx.xlsx\",\n",
    "}\n",
    "\n",
    "def normalize_name(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = s.strip()\n",
    "    s = \"\".join(c for c in unicodedata.normalize(\"NFKD\", s) if not unicodedata.combining(c))\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"\\b(jr|sr|ii|iii|iv|v)\\b\\.?\", \"\", s)\n",
    "    s = re.sub(r\"[^a-z\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "inj_dfs = []\n",
    "for year, path in injury_files.items():\n",
    "    if not path.exists():\n",
    "        print(f\"[WARN] Missing {path}\")\n",
    "        continue\n",
    "    df = pd.read_excel(path)\n",
    "    name_col = next((c for c in df.columns if \"player\" in c.lower() or \"name\" in c.lower()), None)\n",
    "    if name_col is None:\n",
    "        raise ValueError(f\"No name column found in {path}\")\n",
    "    df[\"name_norm\"] = df[name_col].astype(str).map(normalize_name)\n",
    "    df[\"year\"] = year\n",
    "    inj_dfs.append(df[[\"name_norm\",\"year\"]])\n",
    "\n",
    "inj_all = pd.concat(inj_dfs, ignore_index=True).drop_duplicates()\n",
    "print(\"Unified injuries rows:\", len(inj_all))\n",
    "print(\"By year:\")\n",
    "print(inj_all.groupby(\"year\").size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6e86cd-42d3-41f5-8018-ee53ceb1976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "DATA = Path.cwd() if Path.cwd().name == \"data\" else Path.cwd()/\"data\"\n",
    "p_gz = DATA / \"pitches_with_injuries_2021_2023.csv.gz\"\n",
    "assert p_gz.exists(), f\"Missing {p_gz}\"\n",
    "\n",
    "dfm = pd.read_csv(p_gz, low_memory=False, compression=\"gzip\")\n",
    "\n",
    "dfm[\"game_date\"] = pd.to_datetime(dfm.get(\"game_date\"), errors=\"coerce\")\n",
    "if \"game_year\" not in dfm.columns:\n",
    "    dfm[\"game_year\"] = dfm[\"game_date\"].dt.year\n",
    "\n",
    "if \"player_name\" in dfm.columns:\n",
    "    dfm[\"_pname_norm\"] = dfm[\"player_name\"].astype(str).map(normalize_name)\n",
    "elif \"pitcher_name\" in dfm.columns:\n",
    "    dfm[\"_pname_norm\"] = dfm[\"pitcher_name\"].astype(str).map(normalize_name)\n",
    "else:\n",
    "    dfm[\"_pname_norm\"] = \"\"\n",
    "\n",
    "inj_set = set(zip(inj_all[\"name_norm\"], inj_all[\"year\"]))\n",
    "\n",
    "dfm[\"injured\"] = dfm.apply(\n",
    "    lambda row: 1 if (row[\"_pname_norm\"], row[\"game_year\"]) in inj_set else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"Pitch-level injury counts by year:\")\n",
    "print(dfm.groupby(\"game_year\")[\"injured\"].sum())\n",
    "\n",
    "p_labeled = DATA / \"pitches_with_injuries_2021_2023_labeled.csv.gz\"\n",
    "dfm.to_csv(p_labeled, index=False, compression=\"gzip\")\n",
    "print(f\"\\nSaved labeled merged file to: {p_labeled}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41994f6d-e9c8-4754-9574-7e0589ee6b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "DATA = Path.cwd() if Path.cwd().name == \"data\" else Path.cwd()/\"data\"\n",
    "\n",
    "inj_paths = [\n",
    "    DATA / \"2021roster-resource__injury-report.xlsx.xlsx\",\n",
    "    DATA / \"2022roster-resource__injury-report.xlsx\",\n",
    "    DATA / \"2023roster-resource__injury-report.xlsx.xlsx\",\n",
    "]\n",
    "\n",
    "def preview_injury_file(p):\n",
    "    if not p.exists():\n",
    "        print(f\"[MISS] {p}\")\n",
    "        return\n",
    "    df = pd.read_excel(p, nrows=8)\n",
    "    print(f\"\\n=== {p.name} ===\")\n",
    "    print(\"Columns:\", list(df.columns))\n",
    "    like_id = [c for c in df.columns if re.search(r\"mlbam|player.*id|mlb.*id\", str(c), flags=re.I)]\n",
    "    like_name = [c for c in df.columns if re.search(r\"(player|name)\", str(c), flags=re.I)]\n",
    "    show_cols = (like_id + like_name)[:8]\n",
    "    if show_cols:\n",
    "        print(df[show_cols].head(5))\n",
    "    else:\n",
    "        print(df.head(5))\n",
    "\n",
    "for p in inj_paths:\n",
    "    preview_injury_file(p)\n",
    "\n",
    "p_gz = DATA / \"pitches_with_injuries_2021_2023.csv.gz\"\n",
    "dfm = pd.read_csv(p_gz, nrows=5_000, low_memory=False, compression=\"gzip\")\n",
    "print(\"\\n=== Statcast merged: sample columns ===\")\n",
    "print(list(dfm.columns))\n",
    "\n",
    "pitcher_name_candidates = [c for c in dfm.columns if (\"pitch\" in c.lower() and \"name\" in c.lower())]\n",
    "print(\"Pitcher-name candidates in Statcast:\", pitcher_name_candidates)\n",
    "print(\"Has 'player_name' (often batter):\", \"player_name\" in dfm.columns)\n",
    "print(\"Has 'pitcher' (MLBAM ID):\", \"pitcher\" in dfm.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0228dc0-1980-4acc-8801-426ce11c64f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "DATA = Path.cwd() if Path.cwd().name == \"data\" else Path.cwd()/\"data\"\n",
    "p_gz = DATA / \"pitches_with_injuries_2021_2023.csv.gz\"\n",
    "assert p_gz.exists(), f\"Missing {p_gz}\"\n",
    "\n",
    "dfm = pd.read_csv(p_gz, low_memory=False, compression=\"gzip\")\n",
    "dfm[\"game_date\"] = pd.to_datetime(dfm.get(\"game_date\"), errors=\"coerce\")\n",
    "if \"game_year\" not in dfm.columns:\n",
    "    dfm[\"game_year\"] = dfm[\"game_date\"].dt.year\n",
    "\n",
    "dedup_keys = [k for k in [\"game_pk\",\"at_bat_number\",\"pitch_number\"] if k in dfm.columns]\n",
    "if dedup_keys:\n",
    "    dfm = dfm.drop_duplicates(subset=dedup_keys, keep=\"first\")\n",
    "\n",
    "inj_files = {\n",
    "    2021: DATA / \"2021roster-resource__injury-report.xlsx.xlsx\",\n",
    "    2022: DATA / \"2022roster-resource__injury-report.xlsx.xlsx\",\n",
    "    2023: DATA / \"2023roster-resource__injury-report.xlsx.xlsx\",\n",
    "}\n",
    "\n",
    "inj_dfs = []\n",
    "for yr, path in inj_files.items():\n",
    "    if not path.exists():\n",
    "        print(f\"[WARN] Missing {path}\")\n",
    "        continue\n",
    "    inj = pd.read_excel(path, dtype=str)\n",
    "    if \"MLBAMID\" not in inj.columns:\n",
    "        raise ValueError(f\"No MLBAMID col in {path}\")\n",
    "    inj_tmp = inj[[\"MLBAMID\"]].copy()\n",
    "    inj_tmp[\"MLBAMID\"] = inj_tmp[\"MLBAMID\"].astype(str).str.strip()\n",
    "    inj_tmp[\"year\"] = yr\n",
    "    inj_dfs.append(inj_tmp)\n",
    "\n",
    "inj_all = pd.concat(inj_dfs, ignore_index=True).dropna()\n",
    "inj_set = set(map(tuple, inj_all[[\"MLBAMID\",\"year\"]].values))\n",
    "\n",
    "dfm[\"pitcher_str\"] = dfm[\"pitcher\"].astype(str)\n",
    "dfm[\"injured\"] = dfm.apply(\n",
    "    lambda row: 1 if (row[\"pitcher_str\"], row[\"game_year\"]) in inj_set else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"Pitch-level injury counts by year (using MLBAM ID):\")\n",
    "print(dfm.groupby(\"game_year\")[\"injured\"].sum())\n",
    "\n",
    "p_labeled = DATA / \"pitches_with_injuries_2021_2023_labeled.csv.gz\"\n",
    "dfm.to_csv(p_labeled, index=False, compression=\"gzip\")\n",
    "print(f\"\\nSaved labeled merged file to: {p_labeled}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876ef993-de93-4a91-8440-96d82a7c16d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "DATA = Path.cwd() if Path.cwd().name == \"data\" else Path.cwd()/\"data\"\n",
    "p_labeled = DATA / \"pitches_with_injuries_2021_2023_labeled.csv.gz\"\n",
    "assert p_labeled.exists(), f\"Missing {p_labeled}\"\n",
    "\n",
    "dfm = pd.read_csv(p_labeled, low_memory=False, compression=\"gzip\")\n",
    "dfm[\"game_date\"] = pd.to_datetime(dfm[\"game_date\"], errors=\"coerce\")\n",
    "\n",
    "if \"release_spin_rate\" in dfm.columns and \"spin_rate\" not in dfm.columns:\n",
    "    dfm[\"spin_rate\"] = dfm[\"release_spin_rate\"]\n",
    "\n",
    "fastballs = {\"FF\",\"FA\",\"FT\",\"SI\",\"FC\"}\n",
    "breaking  = {\"SL\",\"CU\",\"KC\",\"SV\",\"SC\"}\n",
    "offspeed  = {\"CH\",\"FS\",\"KN\"}\n",
    "\n",
    "def build_features_for_year(df, year):\n",
    "    dfy = df[df[\"game_year\"]==year].copy()\n",
    "    if dfy.empty:\n",
    "        return None\n",
    "    dedup_keys = [k for k in [\"game_pk\",\"at_bat_number\",\"pitch_number\"] if k in dfy.columns]\n",
    "    if dedup_keys:\n",
    "        dfy = dfy.drop_duplicates(subset=dedup_keys, keep=\"first\")\n",
    "    gcols = [\"pitcher\",\"game_pk\",\"game_date\"]\n",
    "    per_game = dfy.groupby(gcols).size().reset_index(name=\"pitches_in_game\")\n",
    "    per_game = per_game.sort_values([\"pitcher\",\"game_date\"])\n",
    "    per_game[\"rest_days\"] = per_game.groupby(\"pitcher\")[\"game_date\"].diff().dt.days\n",
    "    rest_agg = per_game.groupby(\"pitcher\").agg(\n",
    "        games_pitched=(\"game_pk\",\"nunique\"),\n",
    "        total_pitches=(\"pitches_in_game\",\"sum\"),\n",
    "        avg_pitches=(\"pitches_in_game\",\"mean\"),\n",
    "        median_rest=(\"rest_days\",\"median\"),\n",
    "        mean_rest=(\"rest_days\",\"mean\"),\n",
    "        short_rest_games=(\"rest_days\", lambda s: np.sum(s<=3) if s.notna().any() else 0),\n",
    "    ).reset_index()\n",
    "    velo = dfy.groupby(\"pitcher\").agg(\n",
    "        avg_velocity=(\"release_speed\",\"mean\"),\n",
    "        p95_velocity=(\"release_speed\", lambda x: np.nanpercentile(x,95)),\n",
    "        avg_spin=(\"spin_rate\",\"mean\"),\n",
    "    ).reset_index()\n",
    "    tmp = dfy.copy()\n",
    "    tmp[\"pitch_type\"] = tmp[\"pitch_type\"].astype(str).str.upper()\n",
    "    tmp[\"family\"] = np.where(tmp[\"pitch_type\"].isin(fastballs), \"fastball\",\n",
    "                     np.where(tmp[\"pitch_type\"].isin(breaking),  \"breaking\",\n",
    "                     np.where(tmp[\"pitch_type\"].isin(offspeed),  \"offspeed\",\"other\")))\n",
    "    mix = tmp.groupby([\"pitcher\",\"family\"]).size().reset_index(name=\"cnt\")\n",
    "    tot = mix.groupby(\"pitcher\")[\"cnt\"].sum().reset_index(name=\"total\")\n",
    "    mix = mix.merge(tot,on=\"pitcher\",how=\"left\")\n",
    "    mix[\"pct\"] = mix[\"cnt\"]/mix[\"total\"]\n",
    "    mix_pivot = mix.pivot(index=\"pitcher\",columns=\"family\",values=\"pct\").fillna(0).reset_index()\n",
    "    mix_pivot.columns = [\"pitcher\"] + [f\"mix_{c}\" for c in mix_pivot.columns.tolist()[1:]]\n",
    "    lab = dfy.groupby(\"pitcher\")[\"injured\"].max().reset_index().rename(columns={\"injured\":\"injury\"})\n",
    "    feat = rest_agg.merge(velo,on=\"pitcher\",how=\"outer\").merge(mix_pivot,on=\"pitcher\",how=\"outer\").merge(lab,on=\"pitcher\",how=\"left\")\n",
    "    feat[\"injury\"] = feat[\"injury\"].fillna(0).astype(int)\n",
    "    return feat\n",
    "\n",
    "for yr in [2021,2022,2023]:\n",
    "    feat = build_features_for_year(dfm, yr)\n",
    "    if feat is not None:\n",
    "        out = DATA / f\"features_{yr}.csv\"\n",
    "        feat.to_csv(out,index=False)\n",
    "        print(f\"Saved {out}, rows={len(feat)}, positives={feat['injury'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae75fffd-7f82-4397-8d1c-36265bc4c1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "DATA = Path.cwd() if Path.cwd().name == \"data\" else Path.cwd()/\"data\"\n",
    "\n",
    "df21 = pd.read_csv(DATA/\"features_2021.csv\")\n",
    "df22 = pd.read_csv(DATA/\"features_2022.csv\")\n",
    "df23 = pd.read_csv(DATA/\"features_2023.csv\")\n",
    "\n",
    "df21[\"year\"] = 2021\n",
    "df22[\"year\"] = 2022\n",
    "df23[\"year\"] = 2023\n",
    "\n",
    "train = pd.concat([df21,df22], ignore_index=True)\n",
    "test = df23.copy()\n",
    "\n",
    "y_train = train[\"injury\"].fillna(0).astype(int)\n",
    "y_test = test[\"injury\"].fillna(0).astype(int)\n",
    "\n",
    "drop_cols = [\"injury\",\"pitcher\",\"year\"]\n",
    "X_train = train.drop(columns=[c for c in drop_cols if c in train.columns], errors=\"ignore\")\n",
    "X_test  = test.drop(columns=[c for c in drop_cols if c in test.columns], errors=\"ignore\")\n",
    "\n",
    "X_train = X_train.select_dtypes(include=[np.number]).copy()\n",
    "X_test  = X_test.select_dtypes(include=[np.number]).copy()\n",
    "X_train = X_train.fillna(X_train.median(numeric_only=True))\n",
    "X_test  = X_test.fillna(X_train.median(numeric_only=True))\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, \"| positives:\", y_train.sum())\n",
    "print(\"Test shape:\", X_test.shape,  \"| positives:\", y_test.sum())\n",
    "\n",
    "lr_pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"lr\", LogisticRegression(max_iter=1000, class_weight=\"balanced\", solver=\"lbfgs\"))\n",
    "])\n",
    "lr_pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr_pipe.predict(X_test)\n",
    "y_proba = lr_pipe.predict_proba(X_test)[:,1]\n",
    "auc_lr = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "print(\"\\n=== Logistic Regression (train 2021â€“22, test 2023) ===\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "print(f\"ROC AUC: {auc_lr:.3f}\")\n",
    "\n",
    "baseline_lr_report = classification_report(y_test, y_pred, digits=3, output_dict=True)\n",
    "baseline_lr_auc = float(auc_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86c3ab5d-c843-407d-b393-0cb566750588",
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "\nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Users/kavehnaini/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\\n  Referenced from: <6984A3F0-3899-36C4-A85D-20B5520FF130> /Users/kavehnaini/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: tried: '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/Users/kavehnaini/miniconda3/envs/pitcherinjury/lib/python3.10/lib-dynload/../../libomp.dylib' (no such file), '/Users/kavehnaini/miniconda3/envs/pitcherinjury/bin/../lib/libomp.dylib' (no such file)\"]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# === (Step 5) XGBOOST CLASSIFIER (train 2021â€“22, test 2023) ===\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m classification_report, roc_auc_score\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# reuse train/test from Step 4\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages/xgboost/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"XGBoost: eXtreme Gradient Boosting library.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mContributors: https://github.com/dmlc/xgboost/blob/master/CONTRIBUTORS.md\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tracker  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m collective\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     Booster,\n\u001b[1;32m     10\u001b[0m     DataIter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     build_info,\n\u001b[1;32m     16\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages/xgboost/tracker.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01menum\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IntEnum, unique\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, Optional, Union\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _LIB, _check_call, _deprecate_positional_args, make_jcargs\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_family\u001b[39m(addr: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get network family from address.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages/xgboost/core.py:295\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m# load the XGBoost library globally\u001b[39;00m\n\u001b[0;32m--> 295\u001b[0m _LIB \u001b[38;5;241m=\u001b[39m \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_check_call\u001b[39m(ret: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \n\u001b[1;32m    301\u001b[0m \u001b[38;5;124;03m    This function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;124;03m        return value from API calls\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages/xgboost/core.py:257\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m()\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib_success:\n\u001b[1;32m    256\u001b[0m         libname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(lib_paths[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 257\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(\n\u001b[1;32m    258\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;124mXGBoost Library (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlibname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) could not be loaded.\u001b[39m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;124mLikely causes:\u001b[39m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;124m  * OpenMP runtime is not installed\u001b[39m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;124m    - vcomp140.dll or libgomp-1.dll for Windows\u001b[39m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124m    - libomp.dylib for Mac OSX\u001b[39m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124m    - libgomp.so for Linux and other UNIX-like OSes\u001b[39m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124m    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\u001b[39m\n\u001b[1;32m    266\u001b[0m \n\u001b[1;32m    267\u001b[0m \u001b[38;5;124m  * You are running 32-bit Python on a 64-bit OS\u001b[39m\n\u001b[1;32m    268\u001b[0m \n\u001b[1;32m    269\u001b[0m \u001b[38;5;124mError message(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos_error_list\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[1;32m    272\u001b[0m     _register_log_callback(lib)\n\u001b[1;32m    274\u001b[0m     libver \u001b[38;5;241m=\u001b[39m _lib_version(lib)\n",
      "\u001b[0;31mXGBoostError\u001b[0m: \nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Users/kavehnaini/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\\n  Referenced from: <6984A3F0-3899-36C4-A85D-20B5520FF130> /Users/kavehnaini/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: tried: '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/Users/kavehnaini/miniconda3/envs/pitcherinjury/lib/python3.10/lib-dynload/../../libomp.dylib' (no such file), '/Users/kavehnaini/miniconda3/envs/pitcherinjury/bin/../lib/libomp.dylib' (no such file)\"]\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest  = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "neg, pos = (y_train==0).sum(), (y_train==1).sum()\n",
    "scale_pos_weight = neg/pos if pos > 0 else 1\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"eta\": 0.1,\n",
    "    \"max_depth\": 4,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"scale_pos_weight\": scale_pos_weight,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "bst = xgb.train(params, dtrain, num_boost_round=200)\n",
    "\n",
    "y_proba_xgb = bst.predict(dtest)\n",
    "y_pred_xgb = (y_proba_xgb >= 0.5).astype(int)\n",
    "\n",
    "auc_xgb = roc_auc_score(y_test, y_proba_xgb)\n",
    "\n",
    "print(\"\\n=== XGBoost (train 2021â€“22, test 2023) ===\")\n",
    "print(classification_report(y_test, y_pred_xgb, digits=3))\n",
    "print(f\"ROC AUC: {auc_xgb:.3f}\")\n",
    "\n",
    "xgb_auc = float(auc_xgb)\n",
    "xgb_report = classification_report(y_test, y_pred_xgb, digits=3, output_dict=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e132d70d-a209-4c9a-95bb-49974bbce7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost version: 3.0.4\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "print(\"XGBoost version:\", xgboost.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74492939-95f6-412e-86d8-a3cc1b47bf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train positives: 21 / 2300 | scale_pos_weight=108.5\n",
      "\n",
      "=== XGBoost (train 2021â€“22, test 2023) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.986     1.000     0.993      1226\n",
      "           1      0.000     0.000     0.000        17\n",
      "\n",
      "    accuracy                          0.986      1243\n",
      "   macro avg      0.493     0.500     0.497      1243\n",
      "weighted avg      0.973     0.986     0.980      1243\n",
      "\n",
      "ROC AUC: 0.665\n",
      "\n",
      "Top 10 features by gain (approx.):\n",
      "mix_other        0.098793\n",
      "total_pitches    0.088697\n",
      "avg_pitches      0.087084\n",
      "avg_spin         0.086576\n",
      "mean_rest        0.086567\n",
      "p95_velocity     0.085501\n",
      "mix_fastball     0.080656\n",
      "mix_breaking     0.078048\n",
      "median_rest      0.073236\n",
      "games_pitched    0.068969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kavehnaini/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/kavehnaini/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/kavehnaini/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/kavehnaini/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/kavehnaini/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/kavehnaini/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "need_rebuild = False\n",
    "try:\n",
    "    _ = (X_train, X_test, y_train, y_test)\n",
    "except NameError:\n",
    "    need_rebuild = True\n",
    "\n",
    "if need_rebuild:\n",
    "    DATA = Path.cwd() if Path.cwd().name == \"data\" else Path.cwd()/\"data\"\n",
    "    df21 = pd.read_csv(DATA/\"features_2021.csv\"); df21[\"year\"] = 2021\n",
    "    df22 = pd.read_csv(DATA/\"features_2022.csv\"); df22[\"year\"] = 2022\n",
    "    df23 = pd.read_csv(DATA/\"features_2023.csv\"); df23[\"year\"] = 2023\n",
    "    train = pd.concat([df21, df22], ignore_index=True)\n",
    "    test  = df23.copy()\n",
    "    y_train = train[\"injury\"].fillna(0).astype(int)\n",
    "    y_test  = test[\"injury\"].fillna(0).astype(int)\n",
    "    drop_cols = [\"injury\",\"pitcher\",\"year\"]\n",
    "    X_train = train.drop(columns=[c for c in drop_cols if c in train.columns], errors=\"ignore\").select_dtypes(np.number)\n",
    "    X_test  = test.drop(columns=[c for c in drop_cols if c in test.columns], errors=\"ignore\").select_dtypes(np.number)\n",
    "    X_train = X_train.fillna(X_train.median(numeric_only=True))\n",
    "    X_test  = X_test.fillna(X_train.median(numeric_only=True))\n",
    "\n",
    "neg, pos = (y_train==0).sum(), (y_train==1).sum()\n",
    "scale_pos_weight = (neg / pos) if pos > 0 else 1.0\n",
    "print(f\"Train positives: {pos} / {neg+pos} | scale_pos_weight={scale_pos_weight:.1f}\")\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=400,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    n_jobs=-1,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "y_proba = xgb.predict_proba(X_test)[:, 1]\n",
    "y_pred  = (y_proba >= 0.5).astype(int)\n",
    "auc_xgb = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"\\n=== XGBoost (train 2021â€“22, test 2023) ===\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "print(f\"ROC AUC: {auc_xgb:.3f}\")\n",
    "\n",
    "xgb_auc = float(auc_xgb)\n",
    "xgb_report = classification_report(y_test, y_pred, digits=3, output_dict=True)\n",
    "\n",
    "try:\n",
    "    importances = pd.Series(xgb.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "    topk = importances.head(10)\n",
    "    print(\"\\nTop 10 features by gain (approx.):\")\n",
    "    print(topk.to_string())\n",
    "except Exception as e:\n",
    "    print(\"\\n[INFO] Could not compute feature importances:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "936c74d6-dd20-46c1-a687-5f70e2ff209b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.665 | PR AUC: 0.031\n",
      "\n",
      "--- Threshold = 0.50 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.986     1.000     0.993      1226\n",
      "           1      0.000     0.000     0.000        17\n",
      "\n",
      "    accuracy                          0.986      1243\n",
      "   macro avg      0.493     0.500     0.497      1243\n",
      "weighted avg      0.973     0.986     0.980      1243\n",
      "\n",
      "Confusion matrix [tn fp; fn tp]:\n",
      " [[1226    0]\n",
      " [  17    0]]\n",
      "\n",
      "--- Youden's J threshold = 0.001 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.995     0.471     0.640      1226\n",
      "           1      0.021     0.824     0.041        17\n",
      "\n",
      "    accuracy                          0.476      1243\n",
      "   macro avg      0.508     0.647     0.340      1243\n",
      "weighted avg      0.982     0.476     0.632      1243\n",
      "\n",
      "Confusion matrix [tn fp; fn tp]:\n",
      " [[578 648]\n",
      " [  3  14]]\n",
      "\n",
      "--- Best-F1 threshold = 0.016 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.990     0.923     0.955      1226\n",
      "           1      0.050     0.294     0.085        17\n",
      "\n",
      "    accuracy                          0.914      1243\n",
      "   macro avg      0.520     0.608     0.520      1243\n",
      "weighted avg      0.977     0.914     0.943      1243\n",
      "\n",
      "Confusion matrix [tn fp; fn tp]:\n",
      " [[1131   95]\n",
      " [  12    5]]\n",
      "\n",
      "--- Top-k (k=17, matches prevalence) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.986     0.986     0.986      1226\n",
      "           1      0.000     0.000     0.000        17\n",
      "\n",
      "    accuracy                          0.973      1243\n",
      "   macro avg      0.493     0.493     0.493      1243\n",
      "weighted avg      0.973     0.973     0.973      1243\n",
      "\n",
      "Confusion matrix [tn fp; fn tp]:\n",
      " [[1209   17]\n",
      " [  17    0]]\n",
      "\n",
      "[Chosen threshold for later use] 0.016\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    classification_report, roc_auc_score, average_precision_score,\n",
    "    confusion_matrix, precision_recall_curve, roc_curve\n",
    ")\n",
    "\n",
    "proba = y_proba \n",
    "y_true = y_test.values if hasattr(y_test, \"values\") else y_test\n",
    "\n",
    "auc = roc_auc_score(y_true, proba)\n",
    "ap  = average_precision_score(y_true, proba)  # PR-AUC\n",
    "print(f\"ROC AUC: {auc:.3f} | PR AUC: {ap:.3f}\")\n",
    "\n",
    "pred_050 = (proba >= 0.50).astype(int)\n",
    "print(\"\\n--- Threshold = 0.50 ---\")\n",
    "print(classification_report(y_true, pred_050, digits=3, zero_division=0))\n",
    "print(\"Confusion matrix [tn fp; fn tp]:\\n\", confusion_matrix(y_true, pred_050))\n",
    "\n",
    "fpr, tpr, thr = roc_curve(y_true, proba)\n",
    "j_idx = np.argmax(tpr - fpr)\n",
    "thr_j = thr[j_idx]\n",
    "pred_j = (proba >= thr_j).astype(int)\n",
    "print(f\"\\n--- Youden's J threshold = {thr_j:.3f} ---\")\n",
    "print(classification_report(y_true, pred_j, digits=3, zero_division=0))\n",
    "print(\"Confusion matrix [tn fp; fn tp]:\\n\", confusion_matrix(y_true, pred_j))\n",
    "\n",
    "ths = np.unique(np.concatenate([thr, np.linspace(0.05,0.95,91)]))\n",
    "best_f1, best_thr = -1, 0.5\n",
    "for t in ths:\n",
    "    p = (proba >= t).astype(int)\n",
    "    tp = np.sum((p==1) & (y_true==1))\n",
    "    fp = np.sum((p==1) & (y_true==0))\n",
    "    fn = np.sum((p==0) & (y_true==1))\n",
    "    prec = tp/(tp+fp) if (tp+fp)>0 else 0.0\n",
    "    rec  = tp/(tp+fn) if (tp+fn)>0 else 0.0\n",
    "    f1 = 2*prec*rec/(prec+rec) if (prec+rec)>0 else 0.0\n",
    "    if f1 > best_f1:\n",
    "        best_f1, best_thr = f1, t\n",
    "pred_f1 = (proba >= best_thr).astype(int)\n",
    "print(f\"\\n--- Best-F1 threshold = {best_thr:.3f} ---\")\n",
    "print(classification_report(y_true, pred_f1, digits=3, zero_division=0))\n",
    "print(\"Confusion matrix [tn fp; fn tp]:\\n\", confusion_matrix(y_true, pred_f1))\n",
    "\n",
    "k = int(np.sum(y_true==1))\n",
    "order = np.argsort(-proba)  \n",
    "pred_topk = np.zeros_like(y_true)\n",
    "pred_topk[order[:k]] = 1\n",
    "print(f\"\\n--- Top-k (k={k}, matches prevalence) ---\")\n",
    "print(classification_report(y_true, pred_topk, digits=3, zero_division=0))\n",
    "print(\"Confusion matrix [tn fp; fn tp]:\\n\", confusion_matrix(y_true, pred_topk))\n",
    "\n",
    "chosen_threshold = float(best_thr)\n",
    "print(f\"\\n[Chosen threshold for later use] {chosen_threshold:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6c751cc-ba49-4559-88a5-1d284be6cfa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gn/qzws17sj381dyccqftv219180000gn/T/ipykernel_6598/3968739729.py:96: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  bb_early = early.groupby(\"pitcher\").apply(breaking_pct).rename(\"breaking_pct_early\").reset_index()\n",
      "/var/folders/gn/qzws17sj381dyccqftv219180000gn/T/ipykernel_6598/3968739729.py:97: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  bb_season = dfy.groupby(\"pitcher\").apply(breaking_pct).rename(\"breaking_pct_season\").reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/kavehnaini/pitcher-injury-predictor/data/features_2021_enhanced.csv | rows=909 | positives=14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kavehnaini/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages/numpy/lib/_nanfunctions_impl.py:1409: RuntimeWarning: All-NaN slice encountered\n",
      "  return _nanquantile_unchecked(\n",
      "/var/folders/gn/qzws17sj381dyccqftv219180000gn/T/ipykernel_6598/3968739729.py:96: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  bb_early = early.groupby(\"pitcher\").apply(breaking_pct).rename(\"breaking_pct_early\").reset_index()\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Index(...) must be called with a collection of some kind, 'breaking_pct_early' was passed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 112\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m base \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m base\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m enh \u001b[38;5;241m=\u001b[39m \u001b[43madd_enhancements\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# clean\u001b[39;00m\n\u001b[1;32m    114\u001b[0m enh[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minjury\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m enh[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minjury\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 96\u001b[0m, in \u001b[0;36madd_enhancements\u001b[0;34m(df_all, year, base)\u001b[0m\n\u001b[1;32m     94\u001b[0m     pt \u001b[38;5;241m=\u001b[39m d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpitch_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mupper()\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (pt\u001b[38;5;241m.\u001b[39misin(breaking))\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m---> 96\u001b[0m bb_early \u001b[38;5;241m=\u001b[39m \u001b[43mearly\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpitcher\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbreaking_pct\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbreaking_pct_early\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m     97\u001b[0m bb_season \u001b[38;5;241m=\u001b[39m dfy\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpitcher\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(breaking_pct)\u001b[38;5;241m.\u001b[39mrename(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbreaking_pct_season\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m     98\u001b[0m bb \u001b[38;5;241m=\u001b[39m bb_season\u001b[38;5;241m.\u001b[39mmerge(bb_early, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpitcher\u001b[39m\u001b[38;5;124m\"\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages/pandas/core/frame.py:5774\u001b[0m, in \u001b[0;36mDataFrame.rename\u001b[0;34m(self, mapper, index, columns, axis, copy, inplace, level, errors)\u001b[0m\n\u001b[1;32m   5643\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrename\u001b[39m(\n\u001b[1;32m   5644\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5645\u001b[0m     mapper: Renamer \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5653\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5654\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5655\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5656\u001b[0m \u001b[38;5;124;03m    Rename columns or index labels.\u001b[39;00m\n\u001b[1;32m   5657\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5772\u001b[0m \u001b[38;5;124;03m    4  3  6\u001b[39;00m\n\u001b[1;32m   5773\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5774\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rename\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5775\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5776\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5777\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5778\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5780\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5781\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5782\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5783\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages/pandas/core/generic.py:1122\u001b[0m, in \u001b[0;36mNDFrame._rename\u001b[0;34m(self, mapper, index, columns, axis, copy, inplace, level, errors)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39mget_level_values(level)\u001b[38;5;241m.\u001b[39mget_indexer_for(replacements)\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1122\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_indexer_for\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplacements\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(indexer[indexer \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m   1125\u001b[0m     missing_labels \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1126\u001b[0m         label\n\u001b[1;32m   1127\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m index, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(replacements)\n\u001b[1;32m   1128\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m indexer[index] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1129\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages/pandas/core/indexes/base.py:6194\u001b[0m, in \u001b[0;36mIndex.get_indexer_for\u001b[0;34m(self, target)\u001b[0m\n\u001b[1;32m   6176\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6177\u001b[0m \u001b[38;5;124;03mGuaranteed return of an indexer even when non-unique.\u001b[39;00m\n\u001b[1;32m   6178\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6191\u001b[0m \u001b[38;5;124;03marray([0, 2])\u001b[39;00m\n\u001b[1;32m   6192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_as_unique:\n\u001b[0;32m-> 6194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6195\u001b[0m indexer, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_indexer_non_unique(target)\n\u001b[1;32m   6196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indexer\n",
      "File \u001b[0;32m~/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages/pandas/core/indexes/base.py:3887\u001b[0m, in \u001b[0;36mIndex.get_indexer\u001b[0;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[1;32m   3885\u001b[0m method \u001b[38;5;241m=\u001b[39m clean_reindex_fill_method(method)\n\u001b[1;32m   3886\u001b[0m orig_target \u001b[38;5;241m=\u001b[39m target\n\u001b[0;32m-> 3887\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_cast_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3889\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_method(method, limit, tolerance)\n\u001b[1;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_as_unique:\n",
      "File \u001b[0;32m~/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages/pandas/core/indexes/base.py:6729\u001b[0m, in \u001b[0;36mIndex._maybe_cast_listlike_indexer\u001b[0;34m(self, target)\u001b[0m\n\u001b[1;32m   6725\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_maybe_cast_listlike_indexer\u001b[39m(\u001b[38;5;28mself\u001b[39m, target) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Index:\n\u001b[1;32m   6726\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6727\u001b[0m \u001b[38;5;124;03m    Analogue to maybe_cast_indexer for get_indexer instead of get_loc.\u001b[39;00m\n\u001b[1;32m   6728\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 6729\u001b[0m     target_index \u001b[38;5;241m=\u001b[39m \u001b[43mensure_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6730\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   6731\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(target, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6732\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6735\u001b[0m         \u001b[38;5;66;03m# If we started with a list-like, avoid inference to string dtype if self\u001b[39;00m\n\u001b[1;32m   6736\u001b[0m         \u001b[38;5;66;03m# is object dtype (coercing to string dtype will alter the missing values)\u001b[39;00m\n\u001b[1;32m   6737\u001b[0m         target_index \u001b[38;5;241m=\u001b[39m Index(target, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages/pandas/core/indexes/base.py:7715\u001b[0m, in \u001b[0;36mensure_index\u001b[0;34m(index_like, copy)\u001b[0m\n\u001b[1;32m   7713\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Index(index_like, copy\u001b[38;5;241m=\u001b[39mcopy, tupleize_cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   7714\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 7715\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mIndex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages/pandas/core/indexes/base.py:527\u001b[0m, in \u001b[0;36mIndex.__new__\u001b[0;34m(cls, data, dtype, copy, name, tupleize_cols)\u001b[0m\n\u001b[1;32m    524\u001b[0m         data \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39masarray_tuplesafe(data, dtype\u001b[38;5;241m=\u001b[39m_dtype_obj)\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_scalar(data):\n\u001b[0;32m--> 527\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_scalar_data_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__array__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(np\u001b[38;5;241m.\u001b[39masarray(data), dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[0;32m~/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages/pandas/core/indexes/base.py:5299\u001b[0m, in \u001b[0;36mIndex._raise_scalar_data_error\u001b[0;34m(cls, data)\u001b[0m\n\u001b[1;32m   5294\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   5295\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   5296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_raise_scalar_data_error\u001b[39m(\u001b[38;5;28mcls\u001b[39m, data):\n\u001b[1;32m   5297\u001b[0m     \u001b[38;5;66;03m# We return the TypeError so that we can raise it from the constructor\u001b[39;00m\n\u001b[1;32m   5298\u001b[0m     \u001b[38;5;66;03m#  in order to keep mypy happy\u001b[39;00m\n\u001b[0;32m-> 5299\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   5300\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(...) must be called with a collection of some \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5301\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(data)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28misinstance\u001b[39m(data,\u001b[38;5;250m \u001b[39mnp\u001b[38;5;241m.\u001b[39mgeneric)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mstr\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwas passed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5303\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: Index(...) must be called with a collection of some kind, 'breaking_pct_early' was passed"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "DATA = Path.cwd() if Path.cwd().name == \"data\" else Path.cwd()/\"data\"\n",
    "p_labeled = DATA / \"pitches_with_injuries_2021_2023_labeled.csv.gz\"\n",
    "assert p_labeled.exists(), f\"Missing {p_labeled}\"\n",
    "\n",
    "df = pd.read_csv(p_labeled, low_memory=False, compression=\"gzip\")\n",
    "df[\"game_date\"] = pd.to_datetime(df[\"game_date\"], errors=\"coerce\")\n",
    "if \"game_year\" not in df.columns:\n",
    "    df[\"game_year\"] = df[\"game_date\"].dt.year\n",
    "if \"release_spin_rate\" in df.columns and \"spin_rate\" not in df.columns:\n",
    "    df[\"spin_rate\"] = df[\"release_spin_rate\"]\n",
    "if \"pitch_type\" not in df.columns:\n",
    "    df[\"pitch_type\"] = np.nan\n",
    "\n",
    "dedup_keys = [k for k in [\"game_pk\",\"at_bat_number\",\"pitch_number\"] if k in df.columns]\n",
    "if dedup_keys:\n",
    "    df = df.drop_duplicates(subset=dedup_keys, keep=\"first\")\n",
    "\n",
    "fastballs = {\"FF\",\"FA\",\"FT\",\"SI\",\"FC\"}\n",
    "breaking  = {\"SL\",\"CU\",\"KC\",\"SV\",\"SC\"}\n",
    "offspeed  = {\"CH\",\"FS\",\"KN\"}\n",
    "\n",
    "def build_base(dfy):\n",
    "    gcols = [\"pitcher\",\"game_pk\",\"game_date\"]\n",
    "    per_game = dfy.groupby(gcols).size().reset_index(name=\"pitches_in_game\")\n",
    "    per_game = per_game.sort_values([\"pitcher\",\"game_date\"])\n",
    "    per_game[\"rest_days\"] = per_game.groupby(\"pitcher\")[\"game_date\"].diff().dt.days\n",
    "    rest = per_game.groupby(\"pitcher\").agg(\n",
    "        games_pitched=(\"game_pk\",\"nunique\"),\n",
    "        total_pitches=(\"pitches_in_game\",\"sum\"),\n",
    "        avg_pitches=(\"pitches_in_game\",\"mean\"),\n",
    "        median_rest=(\"rest_days\",\"median\"),\n",
    "        mean_rest=(\"rest_days\",\"mean\"),\n",
    "        short_rest_games=(\"rest_days\", lambda s: np.sum(s<=3) if s.notna().any() else 0),\n",
    "    ).reset_index()\n",
    "    vs = dfy.groupby(\"pitcher\").agg(\n",
    "        avg_velocity=(\"release_speed\",\"mean\"),\n",
    "        p95_velocity=(\"release_speed\", lambda x: np.nanpercentile(x,95)),\n",
    "        avg_spin=(\"spin_rate\",\"mean\"),\n",
    "    ).reset_index()\n",
    "    tmp = dfy.copy()\n",
    "    tmp[\"pitch_type\"] = tmp[\"pitch_type\"].astype(str).str.upper()\n",
    "    tmp[\"family\"] = np.where(tmp[\"pitch_type\"].isin(fastballs), \"fastball\",\n",
    "                      np.where(tmp[\"pitch_type\"].isin(breaking),  \"breaking\",\n",
    "                      np.where(tmp[\"pitch_type\"].isin(offspeed),  \"offspeed\",\"other\")))\n",
    "    mix = tmp.groupby([\"pitcher\",\"family\"]).size().reset_index(name=\"cnt\")\n",
    "    tot = mix.groupby(\"pitcher\")[\"cnt\"].sum().reset_index(name=\"total\")\n",
    "    mix = mix.merge(tot,on=\"pitcher\",how=\"left\")\n",
    "    mix[\"pct\"] = mix[\"cnt\"]/mix[\"total\"]\n",
    "    mix_pivot = mix.pivot(index=\"pitcher\",columns=\"family\",values=\"pct\").fillna(0).reset_index()\n",
    "    mix_pivot.columns = [\"pitcher\"] + [f\"mix_{c}\" for c in mix_pivot.columns.tolist()[1:]]\n",
    "    lab = dfy.groupby(\"pitcher\")[\"injured\"].max().reset_index().rename(columns={\"injured\":\"injury\"})\n",
    "    return rest.merge(vs,on=\"pitcher\",how=\"outer\").merge(mix_pivot,on=\"pitcher\",how=\"outer\").merge(lab,on=\"pitcher\",how=\"left\")\n",
    "\n",
    "def add_enhancements(df_all, year, base):\n",
    "    dfy = df_all[df_all[\"game_year\"]==year].copy()\n",
    "    prev = df_all[df_all[\"game_year\"]==year-1]\n",
    "    vel_prev = prev.groupby(\"pitcher\")[\"release_speed\"].mean().rename(\"avg_velocity_prev\").reset_index()\n",
    "    vel_season = dfy.groupby(\"pitcher\")[\"release_speed\"].mean().rename(\"season_velocity\").reset_index()\n",
    "    dfy[\"month\"] = dfy[\"game_date\"].dt.month\n",
    "    early = dfy[dfy[\"month\"].isin([3,4])]\n",
    "    vel_early = early.groupby(\"pitcher\")[\"release_speed\"].mean().rename(\"early_velocity\").reset_index()\n",
    "    vel = vel_season.merge(vel_prev, on=\"pitcher\", how=\"left\").merge(vel_early, on=\"pitcher\", how=\"left\")\n",
    "    vel[\"velocity_delta\"] = np.where(vel[\"avg_velocity_prev\"].notna(),\n",
    "                                     vel[\"season_velocity\"] - vel[\"avg_velocity_prev\"],\n",
    "                                     vel[\"season_velocity\"] - vel[\"early_velocity\"])\n",
    "    daily = (dfy.groupby([\"pitcher\", dfy[\"game_date\"].dt.date]).size()\n",
    "             .reset_index(name=\"pitches\"))\n",
    "    daily = daily.rename(columns={\"game_date\":\"date\"})\n",
    "    daily[\"date\"] = pd.to_datetime(daily[\"date\"])\n",
    "    daily = daily.sort_values([\"pitcher\",\"date\"])\n",
    "    daily[\"roll14\"] = daily.groupby(\"pitcher\")[\"pitches\"].transform(lambda s: s.rolling(14, min_periods=1).sum())\n",
    "    spike = daily.groupby(\"pitcher\")[\"roll14\"].agg(max_14d=\"max\", median_14d=\"median\").reset_index()\n",
    "    spike[\"workload_spike_14d\"] = spike[\"max_14d\"] / spike[\"median_14d\"].replace(0, np.nan)\n",
    "    def breaking_pct(d):\n",
    "        if len(d)==0: return np.nan\n",
    "        pt = d[\"pitch_type\"].astype(str).str.upper()\n",
    "        return (pt.isin(breaking)).mean()\n",
    "    bb_early = early.groupby(\"pitcher\").apply(breaking_pct).rename(\"breaking_pct_early\").reset_index()\n",
    "    bb_season = dfy.groupby(\"pitcher\").apply(breaking_pct).rename(\"breaking_pct_season\").reset_index()\n",
    "    bb = bb_season.merge(bb_early, on=\"pitcher\", how=\"left\")\n",
    "    bb[\"breaking_usage_delta\"] = bb[\"breaking_pct_season\"] - bb[\"breaking_pct_early\"]\n",
    "    enh = (base\n",
    "           .merge(vel[[\"pitcher\",\"velocity_delta\"]], on=\"pitcher\", how=\"left\")\n",
    "           .merge(spike[[\"pitcher\",\"workload_spike_14d\"]], on=\"pitcher\", how=\"left\")\n",
    "           .merge(bb[[\"pitcher\",\"breaking_usage_delta\"]], on=\"pitcher\", how=\"left\"))\n",
    "    return enh\n",
    "\n",
    "enhanced_paths = {}\n",
    "for yr in [2021, 2022, 2023]:\n",
    "    base = build_base(df[df[\"game_year\"]==yr])\n",
    "    if base is None or base.empty:\n",
    "        continue\n",
    "    enh = add_enhancements(df, yr, base)\n",
    "    enh[\"injury\"] = enh[\"injury\"].fillna(0).astype(int)\n",
    "    num_cols = enh.select_dtypes(include=[np.number]).columns\n",
    "    enh[num_cols] = enh[num_cols].replace([np.inf,-np.inf], np.nan)\n",
    "    enh[num_cols] = enh[num_cols].fillna(enh[num_cols].median(numeric_only=True))\n",
    "    out = DATA / f\"features_{yr}_enhanced.csv\"\n",
    "    enh.to_csv(out, index=False)\n",
    "    enhanced_paths[yr] = out\n",
    "    print(f\"Saved {out} | rows={len(enh)} | positives={enh['injury'].sum()}\")\n",
    "\n",
    "df21e = pd.read_csv(enhanced_paths[2021]); df21e[\"year\"] = 2021\n",
    "df22e = pd.read_csv(enhanced_paths[2022]); df22e[\"year\"] = 2022\n",
    "df23e = pd.read_csv(enhanced_paths[2023]); df23e[\"year\"] = 2023\n",
    "\n",
    "train_e = pd.concat([df21e, df22e], ignore_index=True)\n",
    "test_e  = df23e.copy()\n",
    "\n",
    "y_train_e = train_e[\"injury\"].astype(int)\n",
    "y_test_e  = test_e[\"injury\"].astype(int)\n",
    "\n",
    "drop_cols = [\"injury\",\"pitcher\",\"year\"]\n",
    "X_train_e = train_e.drop(columns=[c for c in drop_cols if c in train_e.columns], errors=\"ignore\").select_dtypes(np.number)\n",
    "X_test_e  = test_e.drop(columns=[c for c in drop_cols if c in test_e.columns], errors=\"ignore\").select_dtypes(np.number)\n",
    "\n",
    "X_train_e = X_train_e.fillna(X_train_e.median(numeric_only=True))\n",
    "X_test_e  = X_test_e.fillna(X_train_e.median(numeric_only=True))\n",
    "\n",
    "lr_pipe_e = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"lr\", LogisticRegression(max_iter=1000, class_weight=\"balanced\", solver=\"lbfgs\"))\n",
    "])\n",
    "lr_pipe_e.fit(X_train_e, y_train_e)\n",
    "y_prob_lr_e = lr_pipe_e.predict_proba(X_test_e)[:,1]\n",
    "y_pred_lr_e = (y_prob_lr_e >= 0.5).astype(int)\n",
    "auc_lr_e = roc_auc_score(y_test_e, y_prob_lr_e)\n",
    "print(\"\\n=== Logistic Regression (ENHANCED) ===\")\n",
    "print(classification_report(y_test_e, y_pred_lr_e, digits=3, zero_division=0))\n",
    "print(f\"ROC AUC: {auc_lr_e:.3f}\")\n",
    "\n",
    "neg, pos = (y_train_e==0).sum(), (y_train_e==1).sum()\n",
    "scale_pos_weight = (neg/pos) if pos>0 else 1.0\n",
    "xgb_e = XGBClassifier(\n",
    "    n_estimators=500, learning_rate=0.05, max_depth=4,\n",
    "    subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
    "    objective=\"binary:logistic\", eval_metric=\"auc\",\n",
    "    n_jobs=-1, scale_pos_weight=scale_pos_weight, random_state=42\n",
    ")\n",
    "xgb_e.fit(X_train_e, y_train_e)\n",
    "y_prob_xgb_e = xgb_e.predict_proba(X_test_e)[:,1]\n",
    "y_pred_xgb_e = (y_prob_xgb_e >= 0.5).astype(int)\n",
    "auc_xgb_e = roc_auc_score(y_test_e, y_prob_xgb_e)\n",
    "print(\"\\n=== XGBoost (ENHANCED) ===\")\n",
    "print(classification_report(y_test_e, y_pred_xgb_e, digits=3, zero_division=0))\n",
    "print(f\"ROC AUC: {auc_xgb_e:.3f}\")\n",
    "\n",
    "enh_lr_auc  = float(auc_lr_e)\n",
    "enh_xgb_auc = float(auc_xgb_e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc2fc1ef-4150-4a50-baf2-70f3cb66146a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dedup pitches: 2254254 -> 2254254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gn/qzws17sj381dyccqftv219180000gn/T/ipykernel_6598/333910088.py:114: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  daily_full = daily.groupby(\"pitcher\", group_keys=False).apply(_complete_dates)\n",
      "/var/folders/gn/qzws17sj381dyccqftv219180000gn/T/ipykernel_6598/333910088.py:124: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  bb_season = dfy.groupby(\"pitcher\").apply(_pct_breaking).rename(\"breaking_pct_season\").reset_index()\n",
      "/var/folders/gn/qzws17sj381dyccqftv219180000gn/T/ipykernel_6598/333910088.py:125: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  bb_early  = early.groupby(\"pitcher\").apply(_pct_breaking).rename(\"breaking_pct_early\").reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/kavehnaini/pitcher-injury-predictor/data/features_2021_enhanced.csv | rows=909 | positives=14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kavehnaini/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages/numpy/lib/_nanfunctions_impl.py:1409: RuntimeWarning: All-NaN slice encountered\n",
      "  return _nanquantile_unchecked(\n",
      "/var/folders/gn/qzws17sj381dyccqftv219180000gn/T/ipykernel_6598/333910088.py:114: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  daily_full = daily.groupby(\"pitcher\", group_keys=False).apply(_complete_dates)\n",
      "/var/folders/gn/qzws17sj381dyccqftv219180000gn/T/ipykernel_6598/333910088.py:124: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  bb_season = dfy.groupby(\"pitcher\").apply(_pct_breaking).rename(\"breaking_pct_season\").reset_index()\n",
      "/var/folders/gn/qzws17sj381dyccqftv219180000gn/T/ipykernel_6598/333910088.py:125: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  bb_early  = early.groupby(\"pitcher\").apply(_pct_breaking).rename(\"breaking_pct_early\").reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/kavehnaini/pitcher-injury-predictor/data/features_2022_enhanced.csv | rows=1391 | positives=7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kavehnaini/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages/numpy/lib/_nanfunctions_impl.py:1409: RuntimeWarning: All-NaN slice encountered\n",
      "  return _nanquantile_unchecked(\n",
      "/var/folders/gn/qzws17sj381dyccqftv219180000gn/T/ipykernel_6598/333910088.py:114: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  daily_full = daily.groupby(\"pitcher\", group_keys=False).apply(_complete_dates)\n",
      "/var/folders/gn/qzws17sj381dyccqftv219180000gn/T/ipykernel_6598/333910088.py:124: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  bb_season = dfy.groupby(\"pitcher\").apply(_pct_breaking).rename(\"breaking_pct_season\").reset_index()\n",
      "/var/folders/gn/qzws17sj381dyccqftv219180000gn/T/ipykernel_6598/333910088.py:125: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  bb_early  = early.groupby(\"pitcher\").apply(_pct_breaking).rename(\"breaking_pct_early\").reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/kavehnaini/pitcher-injury-predictor/data/features_2023_enhanced.csv | rows=1243 | positives=17\n",
      "\n",
      "=== Logistic Regression (ENHANCED) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.992     0.686     0.811      1226\n",
      "           1      0.025     0.588     0.049        17\n",
      "\n",
      "    accuracy                          0.685      1243\n",
      "   macro avg      0.509     0.637     0.430      1243\n",
      "weighted avg      0.979     0.685     0.801      1243\n",
      "\n",
      "ROC AUC: 0.641\n",
      "\n",
      "=== XGBoost (ENHANCED) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.986     1.000     0.993      1226\n",
      "           1      0.000     0.000     0.000        17\n",
      "\n",
      "    accuracy                          0.986      1243\n",
      "   macro avg      0.493     0.500     0.497      1243\n",
      "weighted avg      0.973     0.986     0.980      1243\n",
      "\n",
      "ROC AUC: 0.668\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "DATA = Path.cwd() if Path.cwd().name == \"data\" else Path.cwd()/\"data\"\n",
    "p_labeled = DATA / \"pitches_with_injuries_2021_2023_labeled.csv.gz\"\n",
    "assert p_labeled.exists(), f\"Missing {p_labeled}\"\n",
    "\n",
    "df = pd.read_csv(p_labeled, low_memory=False, compression=\"gzip\")\n",
    "df[\"game_date\"] = pd.to_datetime(df[\"game_date\"], errors=\"coerce\")\n",
    "if \"game_year\" not in df.columns:\n",
    "    df[\"game_year\"] = df[\"game_date\"].dt.year\n",
    "if \"release_spin_rate\" in df.columns and \"spin_rate\" not in df.columns:\n",
    "    df[\"spin_rate\"] = df[\"release_spin_rate\"]\n",
    "if \"pitch_type\" not in df.columns:\n",
    "    df[\"pitch_type\"] = np.nan\n",
    "\n",
    "dedup_keys = [k for k in [\"game_pk\",\"at_bat_number\",\"pitch_number\"] if k in df.columns]\n",
    "if dedup_keys:\n",
    "    before = len(df)\n",
    "    df = df.drop_duplicates(subset=dedup_keys, keep=\"first\")\n",
    "    print(f\"Dedup pitches: {before} -> {len(df)}\")\n",
    "\n",
    "fastballs = {\"FF\",\"FA\",\"FT\",\"SI\",\"FC\"}\n",
    "breaking  = {\"SL\",\"CU\",\"KC\",\"SV\",\"SC\"}\n",
    "offspeed  = {\"CH\",\"FS\",\"KN\"}\n",
    "\n",
    "def build_base(dfy):\n",
    "    gcols = [\"pitcher\",\"game_pk\",\"game_date\"]\n",
    "    per_game = dfy.groupby(gcols).size().reset_index(name=\"pitches_in_game\")\n",
    "    per_game = per_game.sort_values([\"pitcher\",\"game_date\"])\n",
    "    per_game[\"rest_days\"] = per_game.groupby(\"pitcher\")[\"game_date\"].diff().dt.days\n",
    "    rest = per_game.groupby(\"pitcher\").agg(\n",
    "        games_pitched=(\"game_pk\",\"nunique\"),\n",
    "        total_pitches=(\"pitches_in_game\",\"sum\"),\n",
    "        avg_pitches=(\"pitches_in_game\",\"mean\"),\n",
    "        median_rest=(\"rest_days\",\"median\"),\n",
    "        mean_rest=(\"rest_days\",\"mean\"),\n",
    "        short_rest_games=(\"rest_days\", lambda s: np.sum(s<=3) if s.notna().any() else 0),\n",
    "    ).reset_index()\n",
    "\n",
    "    vs = dfy.groupby(\"pitcher\").agg(\n",
    "        avg_velocity=(\"release_speed\",\"mean\"),\n",
    "        p95_velocity=(\"release_speed\", lambda x: np.nanpercentile(x, 95)),\n",
    "        avg_spin=(\"spin_rate\",\"mean\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    tmp = dfy.copy()\n",
    "    tmp[\"pitch_type\"] = tmp[\"pitch_type\"].astype(str).str.upper()\n",
    "    tmp[\"family\"] = np.where(tmp[\"pitch_type\"].isin(fastballs), \"fastball\",\n",
    "                      np.where(tmp[\"pitch_type\"].isin(breaking),  \"breaking\",\n",
    "                      np.where(tmp[\"pitch_type\"].isin(offspeed),  \"offspeed\",\"other\")))\n",
    "    mix = tmp.groupby([\"pitcher\",\"family\"]).size().reset_index(name=\"cnt\")\n",
    "    tot = mix.groupby(\"pitcher\")[\"cnt\"].sum().reset_index(name=\"total\")\n",
    "    mix = mix.merge(tot, on=\"pitcher\", how=\"left\")\n",
    "    mix[\"pct\"] = mix[\"cnt\"] / mix[\"total\"].replace(0, np.nan)\n",
    "    mix_pivot = mix.pivot(index=\"pitcher\", columns=\"family\", values=\"pct\").fillna(0).reset_index()\n",
    "    mix_pivot.columns = [\"pitcher\"] + [f\"mix_{c}\" for c in mix_pivot.columns.tolist()[1:]]\n",
    "\n",
    "    lab = dfy.groupby(\"pitcher\")[\"injured\"].max().reset_index().rename(columns={\"injured\":\"injury\"})\n",
    "\n",
    "    base = rest.merge(vs, on=\"pitcher\", how=\"outer\").merge(mix_pivot, on=\"pitcher\", how=\"outer\").merge(lab, on=\"pitcher\", how=\"left\")\n",
    "    return base\n",
    "\n",
    "def add_enhancements(df_all, year, base):\n",
    "    dfy = df_all[df_all[\"game_year\"]==year].copy()\n",
    "    if dfy.empty:\n",
    "        return base.assign(velocity_delta=np.nan, workload_spike_14d=np.nan, breaking_usage_delta=np.nan)\n",
    "\n",
    "    first_date = dfy.groupby(\"pitcher\")[\"game_date\"].transform(\"min\")\n",
    "    early_cutoff = first_date + pd.Timedelta(days=30)\n",
    "    early_mask = dfy[\"game_date\"] <= early_cutoff\n",
    "    early = dfy[early_mask].copy()\n",
    "\n",
    "    prev = df_all[df_all[\"game_year\"]==year-1]\n",
    "    vel_prev = prev.groupby(\"pitcher\")[\"release_speed\"].mean().rename(\"avg_velocity_prev\").reset_index()\n",
    "    vel_season = dfy.groupby(\"pitcher\")[\"release_speed\"].mean().rename(\"season_velocity\").reset_index()\n",
    "    vel_early = early.groupby(\"pitcher\")[\"release_speed\"].mean().rename(\"early_velocity\").reset_index()\n",
    "\n",
    "    vel = vel_season.merge(vel_prev, on=\"pitcher\", how=\"left\").merge(vel_early, on=\"pitcher\", how=\"left\")\n",
    "    vel[\"velocity_delta\"] = np.where(vel[\"avg_velocity_prev\"].notna(),\n",
    "                                     vel[\"season_velocity\"] - vel[\"avg_velocity_prev\"],\n",
    "                                     vel[\"season_velocity\"] - vel[\"early_velocity\"])\n",
    "\n",
    "    daily = (dfy.groupby([\"pitcher\", dfy[\"game_date\"].dt.date]).size()\n",
    "             .reset_index(name=\"pitches\"))\n",
    "    daily = daily.rename(columns={\"game_date\":\"date\"})\n",
    "    daily[\"date\"] = pd.to_datetime(daily[\"date\"])\n",
    "    daily = daily.sort_values([\"pitcher\",\"date\"])\n",
    "    def _complete_dates(g):\n",
    "        idx = pd.date_range(g[\"date\"].min(), g[\"date\"].max(), freq=\"D\")\n",
    "        gg = g.set_index(\"date\").reindex(idx).fillna(0.0)\n",
    "        gg.index.name = \"date\"\n",
    "        gg = gg.rename_axis([\"date\"]).reset_index()\n",
    "        gg[\"pitcher\"] = g[\"pitcher\"].iloc[0]\n",
    "        return gg\n",
    "    daily_full = daily.groupby(\"pitcher\", group_keys=False).apply(_complete_dates)\n",
    "    daily_full[\"roll14\"] = daily_full.groupby(\"pitcher\")[\"pitches\"].transform(lambda s: s.rolling(14, min_periods=1).sum())\n",
    "    spike = daily_full.groupby(\"pitcher\")[\"roll14\"].agg(max_14d=\"max\", median_14d=\"median\").reset_index()\n",
    "    spike[\"workload_spike_14d\"] = spike[\"max_14d\"] / spike[\"median_14d\"].replace(0, np.nan)\n",
    "\n",
    "    def _pct_breaking(dd):\n",
    "        pt = dd[\"pitch_type\"].astype(str).str.upper()\n",
    "        return (pt.isin(breaking)).mean()\n",
    "\n",
    "    bb_season = dfy.groupby(\"pitcher\").apply(_pct_breaking).rename(\"breaking_pct_season\").reset_index()\n",
    "    bb_early  = early.groupby(\"pitcher\").apply(_pct_breaking).rename(\"breaking_pct_early\").reset_index()\n",
    "    bb = bb_season.merge(bb_early, on=\"pitcher\", how=\"left\")\n",
    "    bb[\"breaking_usage_delta\"] = bb[\"breaking_pct_season\"] - bb[\"breaking_pct_early\"]\n",
    "\n",
    "    enh = (base\n",
    "           .merge(vel[[\"pitcher\",\"velocity_delta\"]], on=\"pitcher\", how=\"left\")\n",
    "           .merge(spike[[\"pitcher\",\"workload_spike_14d\"]], on=\"pitcher\", how=\"left\")\n",
    "           .merge(bb[[\"pitcher\",\"breaking_usage_delta\"]], on=\"pitcher\", how=\"left\"))\n",
    "    return enh\n",
    "\n",
    "enhanced_paths = {}\n",
    "for yr in [2021, 2022, 2023]:\n",
    "    base = build_base(df[df[\"game_year\"]==yr])\n",
    "    if base is None or base.empty:\n",
    "        print(f\"[WARN] No data for {yr}\")\n",
    "        continue\n",
    "    enh = add_enhancements(df, yr, base)\n",
    "    enh[\"injury\"] = enh[\"injury\"].fillna(0).astype(int)\n",
    "    num_cols = enh.select_dtypes(include=[np.number]).columns\n",
    "    enh[num_cols] = enh[num_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    enh[num_cols] = enh[num_cols].fillna(enh[num_cols].median(numeric_only=True))\n",
    "    out = DATA / f\"features_{yr}_enhanced.csv\"\n",
    "    enh.to_csv(out, index=False)\n",
    "    enhanced_paths[yr] = out\n",
    "    print(f\"Saved {out} | rows={len(enh)} | positives={enh['injury'].sum()}\")\n",
    "\n",
    "df21e = pd.read_csv(enhanced_paths[2021]); df21e[\"year\"] = 2021\n",
    "df22e = pd.read_csv(enhanced_paths[2022]); df22e[\"year\"] = 2022\n",
    "df23e = pd.read_csv(enhanced_paths[2023]); df23e[\"year\"] = 2023\n",
    "\n",
    "train_e = pd.concat([df21e, df22e], ignore_index=True)\n",
    "test_e  = df23e.copy()\n",
    "\n",
    "y_train_e = train_e[\"injury\"].astype(int)\n",
    "y_test_e  = test_e[\"injury\"].astype(int)\n",
    "\n",
    "drop_cols = [\"injury\",\"pitcher\",\"year\"]\n",
    "X_train_e = train_e.drop(columns=[c for c in drop_cols if c in train_e.columns], errors=\"ignore\").select_dtypes(np.number)\n",
    "X_test_e  = test_e.drop(columns=[c for c in drop_cols if c in test_e.columns], errors=\"ignore\").select_dtypes(np.number)\n",
    "\n",
    "X_train_e = X_train_e.fillna(X_train_e.median(numeric_only=True))\n",
    "X_test_e  = X_test_e.fillna(X_train_e.median(numeric_only=True))\n",
    "\n",
    "lr_pipe_e = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"lr\", LogisticRegression(max_iter=1000, class_weight=\"balanced\", solver=\"lbfgs\")),\n",
    "])\n",
    "lr_pipe_e.fit(X_train_e, y_train_e)\n",
    "y_prob_lr_e = lr_pipe_e.predict_proba(X_test_e)[:, 1]\n",
    "y_pred_lr_e = (y_prob_lr_e >= 0.5).astype(int)\n",
    "auc_lr_e = roc_auc_score(y_test_e, y_prob_lr_e)\n",
    "print(\"\\n=== Logistic Regression (ENHANCED) ===\")\n",
    "print(classification_report(y_test_e, y_pred_lr_e, digits=3, zero_division=0))\n",
    "print(f\"ROC AUC: {auc_lr_e:.3f}\")\n",
    "\n",
    "neg, pos = (y_train_e==0).sum(), (y_train_e==1).sum()\n",
    "scale_pos_weight = (neg/pos) if pos>0 else 1.0\n",
    "xgb_e = XGBClassifier(\n",
    "    n_estimators=500, learning_rate=0.05, max_depth=4,\n",
    "    subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
    "    objective=\"binary:logistic\", eval_metric=\"auc\",\n",
    "    n_jobs=-1, scale_pos_weight=scale_pos_weight, random_state=42\n",
    ")\n",
    "xgb_e.fit(X_train_e, y_train_e)\n",
    "y_prob_xgb_e = xgb_e.predict_proba(X_test_e)[:, 1]\n",
    "y_pred_xgb_e = (y_prob_xgb_e >= 0.5).astype(int)\n",
    "auc_xgb_e = roc_auc_score(y_test_e, y_prob_xgb_e)\n",
    "print(\"\\n=== XGBoost (ENHANCED) ===\")\n",
    "print(classification_report(y_test_e, y_pred_xgb_e, digits=3, zero_division=0))\n",
    "print(f\"ROC AUC: {auc_xgb_e:.3f}\")\n",
    "\n",
    "enh_lr_auc  = float(auc_lr_e)\n",
    "enh_xgb_auc = float(auc_xgb_e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "684c57be-6e0c-491f-81c0-6f317be86a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      " - /Users/kavehnaini/pitcher-injury-predictor/data/RESULTS_AND_LIMITATIONS.md\n",
      " - /Users/kavehnaini/pitcher-injury-predictor/data/METHODS.md\n",
      " - /Users/kavehnaini/pitcher-injury-predictor/data/COMMON_APP_LINE.txt\n",
      "\n",
      "150-char line:\n",
      " Built MLB pitcher-injury model (Statcast+FanGraphs); engineered workload/velo-delta features; best ROC-AUC 0.668 on 2023 holdout.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "DATA = Path.cwd() if Path.cwd().name == \"data\" else Path.cwd()/\"data\"\n",
    "DATA.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _get(v): \n",
    "    return float(globals()[v]) if v in globals() else None\n",
    "\n",
    "metrics = {\n",
    "    \"baseline_lr_auc\": _get(\"baseline_lr_auc\"),\n",
    "    \"xgb_auc\": _get(\"xgb_auc\"),\n",
    "    \"enh_lr_auc\": _get(\"enh_lr_auc\"),\n",
    "    \"enh_xgb_auc\": _get(\"enh_xgb_auc\"),\n",
    "}\n",
    "\n",
    "def _fmt(x):\n",
    "    return f\"{x:.3f}\" if isinstance(x, (int, float)) and x == x else \"N/A\"\n",
    "\n",
    "best_auc = None\n",
    "for k in [\"enh_xgb_auc\",\"xgb_auc\",\"enh_lr_auc\",\"baseline_lr_auc\"]:\n",
    "    if metrics.get(k) is not None:\n",
    "        best_auc = metrics[k]; break\n",
    "\n",
    "results_para = f\"\"\"**Results.** Using Statcast pitch-by-pitch data (2021â€“2023) with RosterResource injury labels, \\\n",
    "I built pitcher-season features (velocity, spin, workload, pitch mix) and trained models with a proper \\\n",
    "temporal split (train: 2021â€“2022; test: 2023). Baseline logistic regression achieved ROC AUC {_fmt(metrics['baseline_lr_auc'])}. \\\n",
    "Gradient boosting (XGBoost) improved discrimination to ROC AUC {_fmt(metrics['xgb_auc'])}, and with engineered featuresâ€”\\\n",
    "velocity deltas, 14-day workload spikes, and breaking-usage changeâ€”the best model reached ROC AUC {_fmt(metrics['enh_xgb_auc'])} \\\n",
    "(on 2023 holdout). Due to extreme class imbalance, threshold tuning (e.g., Youdenâ€™s J / best-F1) is necessary to trade recall vs precision.\"\"\"\n",
    "\n",
    "limitations_para = \"\"\"**Limitations.** Injury is a rare and partially noisy label; RosterResource coverage varies by year. \\\n",
    "Labels are season-level (did a pitcher go on IL this season), which ignores injury timing within the year. \\\n",
    "Feature scope is mostly kinematics/workload; external risk factors (medical history, biomechanics labs, conditioning) are absent. \\\n",
    "A richer label (injury date) and finer-grained rolling features should improve recall without spiking false positives.\"\"\"\n",
    "\n",
    "methods_outline = \"\"\"**Methods (brief).**\n",
    "- Data: MLB Statcast (pybaseball extract) 2021â€“2023; RosterResource injury lists (mapped by MLBAM ID).\n",
    "- Labeling: pitcher-season injury = 1 if pitcher appears on that seasonâ€™s injury list; pitch-level labels aggregated to season via max().\n",
    "- Features: per-game workload & rest days; average & 95th-percentile velocity; average spin; pitch-mix proportions.\n",
    "- Engineered: velocity delta (season vs prior-year or early-season), 14-day workload spike (max/median), breaking-usage delta (seasonâ€“early).\n",
    "- Split: train on 2021â€“2022, test on 2023 (no leakage).\n",
    "- Models: Logistic Regression (balanced), XGBoost (scale_pos_weight); median imputation; standardization for LR; threshold tuning by ROC/PR analysis.\"\"\"\n",
    "\n",
    "common_app_150 = f\"\"\"Built MLB pitcher-injury model (Statcast+FanGraphs); engineered workload/velo-delta features; best ROC-AUC {_fmt(best_auc)} on 2023 holdout.\"\"\"\n",
    "\n",
    "(DATA/\"RESULTS_AND_LIMITATIONS.md\").write_text(results_para + \"\\n\\n\" + limitations_para)\n",
    "(DATA/\"METHODS.md\").write_text(methods_outline)\n",
    "(DATA/\"COMMON_APP_LINE.txt\").write_text(common_app_150)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" -\", (DATA/\"RESULTS_AND_LIMITATIONS.md\").resolve())\n",
    "print(\" -\", (DATA/\"METHODS.md\").resolve())\n",
    "print(\" -\", (DATA/\"COMMON_APP_LINE.txt\").resolve())\n",
    "print(\"\\n150-char line:\\n\", common_app_150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ba6d79a-1b94-405a-89a0-e8a4f4cb895a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: xgboost_enhanced_fitnow | Test ROC AUC (2023): 0.709\n",
      "Chosen threshold (best-F1): 0.007\n",
      "Saved:\n",
      " - /Users/kavehnaini/pitcher-injury-predictor/data/predictions_2023_xgboost_enhanced_fitnow.csv\n",
      " - /Users/kavehnaini/pitcher-injury-predictor/data/predictions_2023_xgboost_enhanced_fitnow_top25.csv\n",
      "\n",
      "Top 10 preview:\n",
      "   pitcher  injury_actual  injury_prob  injury_pred                    model  \\\n",
      "0   502171              0     0.222943            1  xgboost_enhanced_fitnow   \n",
      "1   681911              0     0.149467            1  xgboost_enhanced_fitnow   \n",
      "2   686294              0     0.118790            1  xgboost_enhanced_fitnow   \n",
      "3   687396              0     0.117842            1  xgboost_enhanced_fitnow   \n",
      "4   641540              0     0.108292            1  xgboost_enhanced_fitnow   \n",
      "5   622072              0     0.095002            1  xgboost_enhanced_fitnow   \n",
      "6   682989              0     0.085114            1  xgboost_enhanced_fitnow   \n",
      "7   471911              0     0.076660            1  xgboost_enhanced_fitnow   \n",
      "8   571510              0     0.075359            1  xgboost_enhanced_fitnow   \n",
      "9   641329              0     0.064499            1  xgboost_enhanced_fitnow   \n",
      "\n",
      "   threshold  \n",
      "0   0.006665  \n",
      "1   0.006665  \n",
      "2   0.006665  \n",
      "3   0.006665  \n",
      "4   0.006665  \n",
      "5   0.006665  \n",
      "6   0.006665  \n",
      "7   0.006665  \n",
      "8   0.006665  \n",
      "9   0.006665  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "DATA = Path.cwd() if Path.cwd().name == \"data\" else Path.cwd()/\"data\"\n",
    "DATA.mkdir(parents=True, exist_ok=True)\n",
    "MODELS = (Path.cwd().parent if Path.cwd().name==\"data\" else Path.cwd()) / \"models\"\n",
    "MODELS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    import joblib\n",
    "except Exception:\n",
    "    from sklearn.externals import joblib  # fallback\n",
    "\n",
    "use_enh = (DATA/\"features_2021_enhanced.csv\").exists() and (DATA/\"features_2023_enhanced.csv\").exists()\n",
    "\n",
    "def load_features(enh: bool):\n",
    "    if enh:\n",
    "        df21 = pd.read_csv(DATA/\"features_2021_enhanced.csv\"); df21[\"year\"]=2021\n",
    "        df22 = pd.read_csv(DATA/\"features_2022_enhanced.csv\"); df22[\"year\"]=2022\n",
    "        df23 = pd.read_csv(DATA/\"features_2023_enhanced.csv\"); df23[\"year\"]=2023\n",
    "    else:\n",
    "        df21 = pd.read_csv(DATA/\"features_2021.csv\"); df21[\"year\"]=2021\n",
    "        df22 = pd.read_csv(DATA/\"features_2022.csv\"); df22[\"year\"]=2022\n",
    "        df23 = pd.read_csv(DATA/\"features_2023.csv\"); df23[\"year\"]=2023\n",
    "    train = pd.concat([df21, df22], ignore_index=True)\n",
    "    test = df23.copy()\n",
    "    y_train = train[\"injury\"].astype(int)\n",
    "    y_test  = test[\"injury\"].astype(int)\n",
    "    drop_cols = [\"injury\",\"year\"]\n",
    "    X_train = train.drop(columns=[c for c in drop_cols if c in train.columns], errors=\"ignore\")\n",
    "    X_test  = test.drop(columns=[c for c in drop_cols if c in test.columns], errors=\"ignore\")\n",
    "    # Keep pitcher id for later merge\n",
    "    pitcher_test = test[\"pitcher\"] if \"pitcher\" in test.columns else pd.Series(np.arange(len(test)))\n",
    "    # numeric only + impute\n",
    "    X_train = X_train.select_dtypes(np.number).fillna(X_train.median(numeric_only=True))\n",
    "    X_test  = X_test.select_dtypes(np.number).fillna(X_train.median(numeric_only=True))\n",
    "    return X_train, y_train, X_test, y_test, pitcher_test\n",
    "\n",
    "X_train, y_train, X_test, y_test, pitcher_ids = load_features(use_enh)\n",
    "\n",
    "model_used = None\n",
    "y_proba = None\n",
    "\n",
    "try:\n",
    "    if use_enh and 'xgb_e' in globals():\n",
    "        y_proba = xgb_e.predict_proba(X_test)[:,1]\n",
    "        model_used = \"xgboost_enhanced\"\n",
    "    elif (not use_enh) and 'xgb' in globals():\n",
    "        y_proba = xgb.predict_proba(X_test)[:,1]\n",
    "        model_used = \"xgboost_basic\"\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "if y_proba is None:\n",
    "    from xgboost import XGBClassifier\n",
    "    neg, pos = (y_train==0).sum(), (y_train==1).sum()\n",
    "    spw = (neg/pos) if pos>0 else 1.0\n",
    "    xgb_tmp = XGBClassifier(\n",
    "        n_estimators=500, learning_rate=0.05, max_depth=4,\n",
    "        subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
    "        objective=\"binary:logistic\", eval_metric=\"auc\",\n",
    "        n_jobs=-1, scale_pos_weight=spw, random_state=42\n",
    "    )\n",
    "    xgb_tmp.fit(X_train, y_train)\n",
    "    y_proba = xgb_tmp.predict_proba(X_test)[:,1]\n",
    "    model_used = \"xgboost_enhanced_fitnow\" if use_enh else \"xgboost_basic_fitnow\"\n",
    "    try:\n",
    "        joblib.dump(xgb_tmp, MODELS/f\"{model_used}.joblib\")\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] Could not save model:\", e)\n",
    "\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "print(f\"Model: {model_used} | Test ROC AUC (2023): {auc:.3f}\")\n",
    "\n",
    "fpr, tpr, thr = roc_curve(y_test, y_proba)\n",
    "grid = np.unique(np.concatenate([thr, np.linspace(0.05,0.95,181)]))\n",
    "best_f1, best_thr = -1, 0.5\n",
    "for t in grid:\n",
    "    pred = (y_proba >= t).astype(int)\n",
    "    tp = ((pred==1)&(y_test==1)).sum()\n",
    "    fp = ((pred==1)&(y_test==0)).sum()\n",
    "    fn = ((pred==0)&(y_test==1)).sum()\n",
    "    prec = tp/(tp+fp) if (tp+fp)>0 else 0.0\n",
    "    rec  = tp/(tp+fn) if (tp+fn)>0 else 0.0\n",
    "    f1 = 2*prec*rec/(prec+rec) if (prec+rec)>0 else 0.0\n",
    "    if f1 > best_f1:\n",
    "        best_f1, best_thr = f1, t\n",
    "\n",
    "pred = (y_proba >= best_thr).astype(int)\n",
    "print(f\"Chosen threshold (best-F1): {best_thr:.3f}\")\n",
    "\n",
    "preds = pd.DataFrame({\n",
    "    \"pitcher\": pitcher_ids.values,\n",
    "    \"injury_actual\": y_test.values,\n",
    "    \"injury_prob\": y_proba,\n",
    "    \"injury_pred\": pred,\n",
    "    \"model\": model_used,\n",
    "    \"threshold\": best_thr,\n",
    "})\n",
    "preds = preds.sort_values(\"injury_prob\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "out_full = DATA / f\"predictions_2023_{model_used}.csv\"\n",
    "out_topk = DATA / f\"predictions_2023_{model_used}_top25.csv\"\n",
    "preds.to_csv(out_full, index=False)\n",
    "preds.head(25).to_csv(out_topk, index=False)\n",
    "print(\"Saved:\")\n",
    "print(\" -\", out_full.resolve())\n",
    "print(\" -\", out_topk.resolve())\n",
    "print(\"\\nTop 10 preview:\")\n",
    "print(preds.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c816b8d-2b97-407f-9480-71106d529bbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1419805489.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 10\u001b[0;36m\u001b[0m\n\u001b[0;31m    readme = f\"\"\"# Pitcher Injury Predictor\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import textwrap\n",
    "\n",
    "ROOT = Path.cwd().parent if Path.cwd().name==\"data\" else Path.cwd()\n",
    "DATA = ROOT/\"data\"; DATA.mkdir(exist_ok=True)\n",
    "SCRIPTS = ROOT/\"scripts\"; SCRIPTS.mkdir(exist_ok=True)\n",
    "MODELS = ROOT/\"models\"; MODELS.mkdir(exist_ok=True)\n",
    "\n",
    "readme = f\"\"\"# Pitcher Injury Predictor\n",
    "\n",
    "End-to-end pipeline to predict MLB **pitcher-season** injury risk using Statcast (2021â€“2023) and RosterResource injuries.\n",
    "\n",
    "## Highlights\n",
    "- Features: velocity (avg/p95), spin, workload & rest, pitch mix.\n",
    "- Engineered: **velocity delta**, **14-day workload spike**, **breaking-usage delta**.\n",
    "- Temporal eval: **train 2021â€“2022 â†’ test 2023**.\n",
    "- Best model: gradient boosting (XGBoost). See `data/RESULTS_AND_LIMITATIONS.md`.\n",
    "\n",
    "## Quickstart\n",
    "```bash\n",
    "conda activate pitcherinjury\n",
    "jupyter lab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86804b56-b374-4154-b867-faf3ad0da547",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1419805489.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 10\u001b[0;36m\u001b[0m\n\u001b[0;31m    readme = f\"\"\"# Pitcher Injury Predictor\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import textwrap\n",
    "\n",
    "ROOT = Path.cwd().parent if Path.cwd().name==\"data\" else Path.cwd()\n",
    "DATA = ROOT/\"data\"; DATA.mkdir(exist_ok=True)\n",
    "SCRIPTS = ROOT/\"scripts\"; SCRIPTS.mkdir(exist_ok=True)\n",
    "MODELS = ROOT/\"models\"; MODELS.mkdir(exist_ok=True)\n",
    "\n",
    "readme = f\"\"\"# Pitcher Injury Predictor\n",
    "\n",
    "End-to-end pipeline to predict MLB **pitcher-season** injury risk using Statcast (2021â€“2023) and RosterResource injuries.\n",
    "\n",
    "## Highlights\n",
    "- Features: velocity (avg/p95), spin, workload & rest, pitch mix.\n",
    "- Engineered: **velocity delta**, **14-day workload spike**, **breaking-usage delta**.\n",
    "- Temporal eval: **train 2021â€“2022 â†’ test 2023**.\n",
    "- Best model: gradient boosting (XGBoost). See `data/RESULTS_AND_LIMITATIONS.md`.\n",
    "\n",
    "## Quickstart\n",
    "```bash\n",
    "conda activate pitcherinjury\n",
    "jupyter lab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ce8e529-d15f-4a2f-9309-4df6e8aa823c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (4102527014.py, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[10], line 14\u001b[0;36m\u001b[0m\n\u001b[0;31m    readme_text = textwrap.dedent(\"\"\"\u001b[0m\n\u001b[0m                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import textwrap\n",
    "\n",
    "CWD = Path.cwd()\n",
    "ROOT = CWD.parent if CWD.name == \"data\" else CWD\n",
    "DATA = ROOT / \"data\"\n",
    "SCRIPTS = ROOT / \"scripts\"\n",
    "MODELS = ROOT / \"models\"\n",
    "for p in (DATA, SCRIPTS, MODELS):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "readme_text = textwrap.dedent(\"\"\"\n",
    "# Pitcher Injury Predictor\n",
    "\n",
    "End-to-end pipeline to predict MLB **pitcher-season** injury risk using Statcast (2021â€“2023) and RosterResource injuries.\n",
    "\n",
    "## Highlights\n",
    "- Features: velocity (avg/p95), spin, workload & rest, pitch mix.\n",
    "- Engineered: velocity delta, 14-day workload spike, breaking-usage delta.\n",
    "- Temporal eval: train 2021â€“2022 â†’ test 2023.\n",
    "- Best model: gradient boosting (XGBoost). See `data/RESULTS_AND_LIMITATIONS.md`.\n",
    "\n",
    "## Quickstart\n",
    "\n",
    "```bash\n",
    "conda activate pitcherinjury\n",
    "jupyter lab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b7dd70e-c441-4ebd-984e-6edce4d3407e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import textwrap\n",
    "\n",
    "CWD = Path.cwd()\n",
    "ROOT = CWD.parent if CWD.name == \"data\" else CWD\n",
    "DATA = ROOT / \"data\"\n",
    "SCRIPTS = ROOT / \"scripts\"\n",
    "MODELS = ROOT / \"models\"\n",
    "for p in (DATA, SCRIPTS, MODELS):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "readme_text = textwrap.dedent(\"\"\"\n",
    "# Pitcher Injury Predictor\n",
    "\n",
    "End-to-end pipeline to predict MLB **pitcher-season** injury risk using Statcast (2021â€“2023) and RosterResource injuries.\n",
    "\n",
    "## Highlights\n",
    "- Features: velocity (avg/p95), spin, workload & rest, pitch mix.\n",
    "- Engineered: velocity delta, 14-day workload spike, breaking-usage delta.\n",
    "- Temporal eval: train 2021â€“2022 â†’ test 2023.\n",
    "- Best model: gradient boosting (XGBoost). See `data/RESULTS_AND_LIMITATIONS.md`.\n",
    "\n",
    "## Quickstart\n",
    "\n",
    "```bash\n",
    "conda activate pitcherinjury\n",
    "jupyter lab \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0eff156-3f20-49c4-a268-090cf90a0d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9a55f60-c035-4a92-809f-03d1111b1d50",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (175848011.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[16], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    python scripts/predict_2023.py\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python scripts/predict_2023.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b70267fc-9c5b-4bb1-9b64-5fd98fef0c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using predictions file: /Users/kavehnaini/pitcher-injury-predictor/data/predictions_2023_xgboost_enhanced_fitnow_top25.csv\n",
      "Test size: 25 | Positives: 1 (4.000%)\n",
      "Precision@5: 0.000\n",
      "Precision@10: 0.000\n",
      "Precision@25: 0.040\n",
      "Precision@50: 0.040\n",
      "Precision@100: 0.040\n",
      "Average Precision (PR-AUC): 0.056\n",
      "\n",
      "Actual injured pitchers (2023) and their ranks (sorted by rank):\n",
      " pitcher  injury_prob  rank\n",
      "  669721     0.047814    18\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "DATA = Path.cwd() if Path.cwd().name == \"data\" else Path.cwd()/\"data\"\n",
    "\n",
    "cands = sorted(DATA.glob(\"predictions_2023_*.csv\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "assert cands, \"No predictions_2023_*.csv found. Run G2 first.\"\n",
    "pred_path = cands[0]\n",
    "print(\"Using predictions file:\", pred_path)\n",
    "\n",
    "preds = pd.read_csv(pred_path)\n",
    "preds = preds.sort_values(\"injury_prob\", ascending=False).reset_index(drop=True)\n",
    "preds[\"rank\"] = np.arange(1, len(preds)+1)\n",
    "\n",
    "y_true = preds[\"injury_actual\"].astype(int).values\n",
    "y_score = preds[\"injury_prob\"].values\n",
    "n_pos = int(y_true.sum())\n",
    "n_all = len(y_true)\n",
    "prevalence = n_pos / n_all if n_all else 0\n",
    "\n",
    "print(f\"Test size: {n_all} | Positives: {n_pos} ({prevalence:.3%})\")\n",
    "\n",
    "def precision_at_k(k):\n",
    "    k = min(k, n_all)\n",
    "    return float((preds.head(k)[\"injury_actual\"] == 1).mean() if k>0 else 0.0)\n",
    "\n",
    "for k in [5, 10, 25, 50, 100]:\n",
    "    print(f\"Precision@{k}: {precision_at_k(k):.3f}\")\n",
    "\n",
    "ap = average_precision_score(y_true, y_score) if n_pos > 0 else float(\"nan\")\n",
    "print(f\"Average Precision (PR-AUC): {ap:.3f}\")\n",
    "\n",
    "pos_rows = preds[preds[\"injury_actual\"] == 1][[\"pitcher\",\"injury_prob\",\"rank\"]]\n",
    "print(\"\\nActual injured pitchers (2023) and their ranks (sorted by rank):\")\n",
    "print(pos_rows.sort_values(\"rank\").to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "314b5507-1e8e-41c3-b1f6-1724e4b5595c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering player lookup table. This may take a moment.\n",
      "Saved: /Users/kavehnaini/pitcher-injury-predictor/data/predictions_2023_top25_with_names.csv\n",
      "   pitcher  injury_actual  injury_prob  injury_pred                    model  \\\n",
      "0   502171              0     0.222943            1  xgboost_enhanced_fitnow   \n",
      "1   681911              0     0.149467            1  xgboost_enhanced_fitnow   \n",
      "2   686294              0     0.118790            1  xgboost_enhanced_fitnow   \n",
      "3   687396              0     0.117842            1  xgboost_enhanced_fitnow   \n",
      "4   641540              0     0.108292            1  xgboost_enhanced_fitnow   \n",
      "5   622072              0     0.095002            1  xgboost_enhanced_fitnow   \n",
      "6   682989              0     0.085114            1  xgboost_enhanced_fitnow   \n",
      "7   471911              0     0.076660            1  xgboost_enhanced_fitnow   \n",
      "8   571510              0     0.075359            1  xgboost_enhanced_fitnow   \n",
      "9   641329              0     0.064499            1  xgboost_enhanced_fitnow   \n",
      "\n",
      "   threshold  rank        full_name  \n",
      "0   0.006665     1        Alex Cobb  \n",
      "1   0.006665     2       Alex Vesia  \n",
      "2   0.006665     3  Amos Willingham  \n",
      "3   0.006665     4   Brent Headrick  \n",
      "4   0.006665     5     Dane Dunning  \n",
      "5   0.006665     6        Alex Wood  \n",
      "6   0.006665     7   Victor Mederos  \n",
      "7   0.006665     8  Carlos Carrasco  \n",
      "8   0.006665     9        Matt Boyd  \n",
      "9   0.006665    10      Bryan Baker  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "DATA = Path.cwd() if Path.cwd().name == \"data\" else Path.cwd()/\"data\"\n",
    "\n",
    "try:\n",
    "    preds\n",
    "except NameError:\n",
    "    cands = sorted(DATA.glob(\"predictions_2023_*.csv\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    assert cands, \"No predictions_2023_*.csv found.\"\n",
    "    preds = pd.read_csv(cands[0]).sort_values(\"injury_prob\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "K = 25   # change this if you want a different list size\n",
    "topk = preds.head(K).copy()\n",
    "\n",
    "def add_names(df):\n",
    "    try:\n",
    "        from pybaseball import playerid_reverse_lookup\n",
    "        ids = df[\"pitcher\"].dropna().astype(int).unique().tolist()\n",
    "        if not ids:\n",
    "            return df\n",
    "        names = playerid_reverse_lookup(ids, key_type=\"mlbam\")\n",
    "        if \"name_first\" in names.columns and \"name_last\" in names.columns:\n",
    "            names[\"full_name\"] = names[\"name_first\"].str.title() + \" \" + names[\"name_last\"].str.title()\n",
    "        elif \"name_use\" in names.columns:\n",
    "            names[\"full_name\"] = names[\"name_use\"]\n",
    "        else:\n",
    "            name_cols = [c for c in names.columns if \"name\" in c.lower()]\n",
    "            if name_cols:\n",
    "                names[\"full_name\"] = names[name_cols[0]]\n",
    "            else:\n",
    "                names[\"full_name\"] = \"\"\n",
    "        key_col = \"key_mlbam\" if \"key_mlbam\" in names.columns else (\"mlbam\" if \"mlbam\" in names.columns else None)\n",
    "        if key_col is None:\n",
    "            return df\n",
    "        out = df.merge(names[[key_col,\"full_name\"]], left_on=\"pitcher\", right_on=key_col, how=\"left\")\n",
    "        out = out.drop(columns=[c for c in [\"key_mlbam\",\"mlbam\"] if c in out.columns])\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        print(\"[INFO] Could not enrich names:\", e)\n",
    "        return df\n",
    "\n",
    "topk_named = add_names(topk)\n",
    "out_topk_named = DATA / \"predictions_2023_top25_with_names.csv\"\n",
    "topk_named.to_csv(out_topk_named, index=False)\n",
    "print(\"Saved:\", out_topk_named.resolve())\n",
    "print(topk_named.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a510711-2c6e-4bd6-a27f-fed1ae62d268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF saved to: /Users/kavehnaini/pitcher-injury-predictor/data/pitcher_injury_summary.pdf\n",
      "Top-25 with names saved to: /Users/kavehnaini/pitcher-injury-predictor/data/predictions_2023_top25_named.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np\n",
    "from sklearn.metrics import (\n",
    "    roc_curve, auc, roc_auc_score, precision_recall_curve,\n",
    "    average_precision_score, confusion_matrix\n",
    ")\n",
    "import textwrap\n",
    "\n",
    "AUTHOR_NAME   = \"Your Name\"           # <-- EDIT\n",
    "CONTACT_EMAIL = \"you@example.com\"     # <-- EDIT\n",
    "GITHUB_URL    = \"https://github.com/<you>/pitcher-injury-predictor\"  # <-- EDIT if you want\n",
    "DEMO_URL      = \"streamlit: run locally; deploy optional\"            # <-- optional\n",
    "\n",
    "DATA = Path.cwd() if Path.cwd().name == \"data\" else Path.cwd()/\"data\"\n",
    "DATA.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "cands = sorted(DATA.glob(\"predictions_2023_*.csv\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "assert cands, \"No predictions_2023_*.csv found. Run G2 first to export predictions.\"\n",
    "pred_path = cands[0]\n",
    "preds = pd.read_csv(pred_path).sort_values(\"injury_prob\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "y_true = preds[\"injury_actual\"].astype(int).values\n",
    "y_score = preds[\"injury_prob\"].values\n",
    "n_pos = int(y_true.sum()); n_all = len(y_true)\n",
    "prev = n_pos / n_all if n_all else 0\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_score) if n_pos>0 and n_all>0 else float(\"nan\")\n",
    "pr_auc  = average_precision_score(y_true, y_score) if n_pos>0 else float(\"nan\")\n",
    "\n",
    "fpr, tpr, thr = roc_curve(y_true, y_score)\n",
    "grid = np.unique(np.concatenate([thr, np.linspace(0.01, 0.99, 199)]))\n",
    "def f1_at(t):\n",
    "    p = (y_score >= t).astype(int)\n",
    "    tp = ((p==1)&(y_true==1)).sum(); fp = ((p==1)&(y_true==0)).sum(); fn = ((p==0)&(y_true==1)).sum()\n",
    "    prec = tp/(tp+fp) if (tp+fp)>0 else 0.0\n",
    "    rec  = tp/(tp+fn) if (tp+fn)>0 else 0.0\n",
    "    return (2*prec*rec/(prec+rec)) if (prec+rec)>0 else 0.0\n",
    "best_f1, best_thr = max((f1_at(t), t) for t in grid)\n",
    "j_idx = np.argmax(tpr - fpr)\n",
    "j_thr = thr[j_idx] if len(thr)>0 else 0.5\n",
    "\n",
    "def prec_at_k(df, k):\n",
    "    k = min(k, len(df))\n",
    "    return float((df.head(k)[\"injury_actual\"]==1).mean()) if k>0 else 0.0\n",
    "\n",
    "p5 = prec_at_k(preds, 5)\n",
    "p10 = prec_at_k(preds, 10)\n",
    "p25 = prec_at_k(preds, 25)\n",
    "p50 = prec_at_k(preds, 50)\n",
    "p100 = prec_at_k(preds, 100)\n",
    "\n",
    "def cm_at(t):\n",
    "    p = (y_score >= t).astype(int)\n",
    "    return confusion_matrix(y_true, p)\n",
    "\n",
    "cm_best = cm_at(best_thr)\n",
    "cm_j    = cm_at(j_thr)\n",
    "\n",
    "topk = preds.head(25).copy()\n",
    "def add_names(df):\n",
    "    try:\n",
    "        from pybaseball import playerid_reverse_lookup\n",
    "        ids = df[\"pitcher\"].dropna().astype(int).unique().tolist()\n",
    "        names = playerid_reverse_lookup(ids, key_type=\"mlbam\")\n",
    "        if \"name_first\" in names.columns and \"name_last\" in names.columns:\n",
    "            names[\"full_name\"] = names[\"name_first\"].str.title() + \" \" + names[\"name_last\"].str.title()\n",
    "        elif \"name_use\" in names.columns:\n",
    "            names[\"full_name\"] = names[\"name_use\"]\n",
    "        else:\n",
    "            name_cols = [c for c in names.columns if \"name\" in c.lower()]\n",
    "            names[\"full_name\"] = names[name_cols[0]] if name_cols else \"\"\n",
    "        key_col = \"key_mlbam\" if \"key_mlbam\" in names.columns else (\"mlbam\" if \"mlbam\" in names.columns else None)\n",
    "        if key_col is None: \n",
    "            return df\n",
    "        out = df.merge(names[[key_col,\"full_name\"]], left_on=\"pitcher\", right_on=key_col, how=\"left\")\n",
    "        out = out.drop(columns=[c for c in [\"key_mlbam\",\"mlbam\"] if c in out.columns])\n",
    "        out[\"display_name\"] = out[\"full_name\"].fillna(\"\").where(out[\"full_name\"].notna() & (out[\"full_name\"]!=\"\"), out[\"pitcher\"].astype(str))\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        df[\"display_name\"] = df[\"pitcher\"].astype(str)\n",
    "        return df\n",
    "\n",
    "topk = add_names(topk)\n",
    "topk_out = DATA/\"predictions_2023_top25_named.csv\"\n",
    "topk[[\"pitcher\",\"display_name\",\"injury_prob\",\"injury_actual\"]].to_csv(topk_out, index=False)\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.backends.backend_pdf import PdfPages\n",
    "except ImportError:\n",
    "    raise SystemExit(\"matplotlib is required. Install it with: pip install matplotlib\")\n",
    "\n",
    "pdf_path = DATA / \"pitcher_injury_summary.pdf\"\n",
    "with PdfPages(pdf_path) as pdf:\n",
    "    fig = plt.figure(figsize=(8.5, 11))\n",
    "    fig.text(0.1, 0.92, \"MLB Pitcher Injury Prediction (2021â€“2023)\", fontsize=20, weight=\"bold\")\n",
    "    fig.text(0.1, 0.88, f\"Author: {AUTHOR_NAME}   â€¢   Contact: {CONTACT_EMAIL}\", fontsize=11)\n",
    "    if GITHUB_URL: fig.text(0.1, 0.85, f\"GitHub: {GITHUB_URL}\", fontsize=11)\n",
    "    if DEMO_URL:   fig.text(0.1, 0.82, f\"Demo: {DEMO_URL}\", fontsize=11)\n",
    "\n",
    "    abstract = f\"\"\"\n",
    "    We predict pitcher-season injury risk using MLB Statcast (2021â€“2023) and RosterResource IL lists.\n",
    "    Features include workload & rest, velocity (avg / p95), spin, and pitch mix. Engineered features add\n",
    "    velocity deltas vs prior year or early-season, 14-day workload spikes, and breaking-usage change.\n",
    "    Trained on 2021â€“2022 and evaluated on 2023 holdout.\n",
    "\n",
    "    Test ROC AUC (2023): {roc_auc:.3f}    â€¢    PR-AUC: {pr_auc:.3f}    â€¢    Prevalence: {prev:.2%}\n",
    "    \"\"\"\n",
    "    abstract = textwrap.dedent(abstract).strip()\n",
    "    fig.text(0.1, 0.74, abstract, fontsize=11, va=\"top\")\n",
    "    rl = (DATA/\"RESULTS_AND_LIMITATIONS.md\")\n",
    "    if rl.exists():\n",
    "        txt = rl.read_text()\n",
    "        txt_wrapped = textwrap.fill(txt, width=100)\n",
    "        fig.text(0.1, 0.68, \"Results & Limitations (summary):\", fontsize=13, weight=\"bold\")\n",
    "        fig.text(0.1, 0.66, txt_wrapped[:1800] + (\"...\" if len(txt_wrapped)>1800 else \"\"), fontsize=9, va=\"top\")\n",
    "    pdf.savefig(fig); plt.close(fig)\n",
    "\n",
    "    fig = plt.figure(figsize=(8.5, 11))\n",
    "    fig.text(0.1, 0.94, \"Methods (brief)\", fontsize=16, weight=\"bold\")\n",
    "    methods = \"\"\"\n",
    "    â€¢ Data: Statcast 2021â€“2023; RosterResource injuries (mapped by MLBAM ID).\n",
    "    â€¢ Labels: season-level injury = 1 if the pitcher appears that season; aggregated pitchâ†’season by max().\n",
    "    â€¢ Features: per-game workload & rest; avg & p95 velocity; avg spin; pitch-mix %.\n",
    "    â€¢ Engineered: velocity delta (vs prior-year or early-season), 14-day workload spike (max/median),\n",
    "      breaking-usage delta (season â€“ early).\n",
    "    â€¢ Split: train 2021â€“2022 â†’ test 2023 (no leakage).\n",
    "    â€¢ Models: Logistic Regression (balanced) and XGBoost (scale_pos_weight); median imputation.\n",
    "    â€¢ Thresholding: tune via ROC (Youdenâ€™s J) or Best-F1; also report precision@K.\n",
    "    \"\"\"\n",
    "    fig.text(0.1, 0.90, textwrap.dedent(methods).strip(), fontsize=10, va=\"top\")\n",
    "\n",
    "    cols = [\"K\",\"Precision\"]\n",
    "    tbl = pd.DataFrame([\n",
    "        (5,  p5), (10, p10), (25, p25), (50, p50), (100, p100)\n",
    "    ], columns=cols)\n",
    "    ax = fig.add_axes([0.10, 0.55, 0.35, 0.25])\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(\"Precision@K (2023)\", fontsize=12, pad=8)\n",
    "    ax.table(cellText=[(int(k), f\"{v:.3f}\") for k,v in zip(tbl[\"K\"], tbl[\"Precision\"])],\n",
    "             colLabels=cols, loc=\"center\")\n",
    "    ax2 = fig.add_axes([0.55, 0.55, 0.35, 0.25])\n",
    "    ax2.axis(\"off\")\n",
    "    ax2.set_title(f\"Confusion @ Best-F1 (t={best_thr:.3f})\", fontsize=12, pad=8)\n",
    "    tn, fp, fn, tp = cm_best.ravel()\n",
    "    ax2.table(cellText=[[\"TN\", tn], [\"FP\", fp], [\"FN\", fn], [\"TP\", tp]],\n",
    "              colLabels=[\"\", \"Count\"], loc=\"center\")\n",
    "    pdf.savefig(fig); plt.close(fig)\n",
    "\n",
    "    fig = plt.figure(figsize=(8.5, 11))\n",
    "    fig.text(0.1, 0.94, \"Discrimination Curves (2023)\", fontsize=16, weight=\"bold\")\n",
    "    ax = fig.add_axes([0.12, 0.56, 0.75, 0.32])\n",
    "    ax.plot(fpr, tpr, label=f\"ROC AUC = {roc_auc:.3f}\")\n",
    "    ax.plot([0,1],[0,1],\"--\", alpha=0.5)\n",
    "    ax.set_xlabel(\"False Positive Rate\"); ax.set_ylabel(\"True Positive Rate\"); ax.set_title(\"ROC\")\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    prec, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "    ax2 = fig.add_axes([0.12, 0.12, 0.75, 0.32])\n",
    "    ax2.plot(recall, prec, label=f\"PR AUC = {pr_auc:.3f}\")\n",
    "    ax2.set_xlabel(\"Recall\"); ax2.set_ylabel(\"Precision\"); ax2.set_title(\"Precisionâ€“Recall\")\n",
    "    ax2.legend(loc=\"upper right\")\n",
    "    pdf.savefig(fig); plt.close(fig)\n",
    "\n",
    "    fig = plt.figure(figsize=(8.5, 11))\n",
    "    fig.text(0.1, 0.94, \"Top-25 Predicted Injury Risk (2023)\", fontsize=16, weight=\"bold\")\n",
    "    cols = [\"Rank\",\"Player\",\"MLBAM\",\"Prob\",\"Actual\"]\n",
    "    tab = pd.DataFrame({\n",
    "        \"Rank\": np.arange(1, min(26, len(topk))+1),\n",
    "        \"Player\": topk[\"display_name\"].fillna(\"\").str.slice(0,24),\n",
    "        \"MLBAM\": topk[\"pitcher\"].astype(str),\n",
    "        \"Prob\": topk[\"injury_prob\"].map(lambda x: f\"{x:.3f}\"),\n",
    "        \"Actual\": topk[\"injury_actual\"].astype(int).astype(str),\n",
    "    })\n",
    "    half = int(np.ceil(len(tab)/2))\n",
    "    left, right = tab.iloc[:half], tab.iloc[half:]\n",
    "    ax = fig.add_axes([0.08, 0.12, 0.40, 0.78]); ax.axis(\"off\"); ax.set_title(\"Top-25 (1â€“{})\".format(half), fontsize=12)\n",
    "    ax.table(cellText=left.values, colLabels=cols, loc=\"center\")\n",
    "    ax2 = fig.add_axes([0.52, 0.12, 0.40, 0.78]); ax2.axis(\"off\"); ax2.set_title(\"Top-25 ({}â€“25)\".format(half+1), fontsize=12)\n",
    "    if not right.empty:\n",
    "        ax2.table(cellText=right.values, colLabels=cols, loc=\"center\")\n",
    "    pdf.savefig(fig); plt.close(fig)\n",
    "\n",
    "print(\"PDF saved to:\", pdf_path.resolve())\n",
    "print(\"Top-25 with names saved to:\", topk_out.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "201db6b9-facf-4f30-851e-9a52799c325b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /Users/kavehnaini/pitcher-injury-predictor/app.py\n",
      "\n",
      "Next steps:\n",
      "1) pip install streamlit\n",
      "2) streamlit run app.py\n",
      "   (It will open a local URL; move the threshold slider and Top-K)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import textwrap, sys\n",
    "\n",
    "ROOT = Path.cwd().parent if Path.cwd().name==\"data\" else Path.cwd()\n",
    "app_py = ROOT / \"app.py\"\n",
    "app_text = textwrap.dedent(\"\"\"\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "import streamlit as st\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix\n",
    "\n",
    "st.set_page_config(page_title=\"Pitcher Injury Predictor\", layout=\"wide\")\n",
    "\n",
    "DATA = Path(__file__).resolve().parent / \"data\"\n",
    "cands = sorted(DATA.glob(\"predictions_2023_*.csv\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "if not cands:\n",
    "    st.error(\"No predictions_2023_*.csv found. Run the notebook G2 cell first.\")\n",
    "    st.stop()\n",
    "\n",
    "pred_path = cands[0]\n",
    "preds = pd.read_csv(pred_path).sort_values(\"injury_prob\", ascending=False).reset_index(drop=True)\n",
    "st.caption(f\"Using: {pred_path.name}\")\n",
    "\n",
    "# Optional: enrich names via pybaseball\n",
    "def add_names(df):\n",
    "    try:\n",
    "        from pybaseball import playerid_reverse_lookup\n",
    "        ids = df[\"pitcher\"].dropna().astype(int).unique().tolist()\n",
    "        names = playerid_reverse_lookup(ids, key_type=\"mlbam\")\n",
    "        if \"name_first\" in names.columns and \"name_last\" in names.columns:\n",
    "            names[\"full_name\"] = names[\"name_first\"].str.title() + \" \" + names[\"name_last\"].str.title()\n",
    "        elif \"name_use\" in names.columns:\n",
    "            names[\"full_name\"] = names[\"name_use\"]\n",
    "        else:\n",
    "            name_cols = [c for c in names.columns if \"name\" in c.lower()]\n",
    "            names[\"full_name\"] = names[name_cols[0]] if name_cols else \"\"\n",
    "        key_col = \"key_mlbam\" if \"key_mlbam\" in names.columns else (\"mlbam\" if \"mlbam\" in names.columns else None)\n",
    "        if key_col is None:\n",
    "            df[\"player_name\"] = df[\"pitcher\"].astype(str)\n",
    "            return df\n",
    "        out = df.merge(names[[key_col,\"full_name\"]], left_on=\"pitcher\", right_on=key_col, how=\"left\")\n",
    "        out[\"player_name\"] = out[\"full_name\"].fillna(out[\"pitcher\"].astype(str))\n",
    "        out = out.drop(columns=[c for c in [\"key_mlbam\",\"mlbam\",\"full_name\"] if c in out.columns])\n",
    "        return out\n",
    "    except Exception:\n",
    "        df[\"player_name\"] = df[\"pitcher\"].astype(str)\n",
    "        return df\n",
    "\n",
    "preds = add_names(preds)\n",
    "\n",
    "y_true = preds[\"injury_actual\"].astype(int).values\n",
    "y_score = preds[\"injury_prob\"].values\n",
    "if (y_true.sum() > 0) and (len(y_true) > 0):\n",
    "    st.sidebar.metric(\"ROC AUC (2023)\", f\"{roc_auc_score(y_true, y_score):.3f}\")\n",
    "    try:\n",
    "        st.sidebar.metric(\"PR AUC\", f\"{average_precision_score(y_true, y_score):.3f}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "st.title(\"Pitcher Injury Predictor â€” 2023\")\n",
    "thr = st.sidebar.slider(\"Decision threshold\", min_value=0.0, max_value=0.2, value=0.01, step=0.001)\n",
    "k = st.sidebar.number_input(\"Show Top-K\", min_value=5, max_value=200, value=25, step=5)\n",
    "\n",
    "preds[\"pred\"] = (preds[\"injury_prob\"] >= thr).astype(int)\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, preds[\"pred\"]).ravel()\n",
    "st.sidebar.write(\"Confusion matrix\")\n",
    "st.sidebar.write(pd.DataFrame({\"\": [\"TN\",\"FP\",\"FN\",\"TP\"], \"Count\":[tn,fp,fn,tp]}))\n",
    "\n",
    "topk = preds.sort_values(\"injury_prob\", ascending=False).head(int(k)).copy()\n",
    "topk_display = topk[[\"player_name\",\"pitcher\",\"injury_prob\",\"injury_actual\",\"pred\"]]\n",
    "topk_display.columns = [\"Player\",\"MLBAM\",\"Injury Prob\",\"Actual\",\"Pred\"]\n",
    "st.subheader(f\"Top {int(k)} Highest-Risk Pitchers\")\n",
    "st.dataframe(topk_display.style.format({\"Injury Prob\":\"{:.3f}\"}), use_container_width=True)\n",
    "\"\"\").strip()\n",
    "\n",
    "app_py.write_text(app_text)\n",
    "print(\"Wrote:\", app_py.resolve())\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1) pip install streamlit\")\n",
    "print(\"2) streamlit run app.py\")\n",
    "print(\"   (It will open a local URL; move the threshold slider and Top-K)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3bbb2ca9-57f9-40e6-865e-c30ac04775c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2564574216.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[23], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    streamlit run app.py\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f407aae-bed3-4bf9-a524-a2449cba21b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Downloading streamlit-1.49.1-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting altair!=5.4.0,!=5.4.1,<6,>=4.0 (from streamlit)\n",
      "  Downloading altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting blinker<2,>=1.5.0 (from streamlit)\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting cachetools<7,>=4.0 (from streamlit)\n",
      "  Downloading cachetools-6.2.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting click<9,>=7.0 (from streamlit)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in /Users/kavehnaini/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages (from streamlit) (2.2.6)\n",
      "Requirement already satisfied: packaging<26,>=20 in /Users/kavehnaini/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages (from streamlit) (25.0)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in /Users/kavehnaini/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages (from streamlit) (2.3.2)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in /Users/kavehnaini/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages (from streamlit) (11.3.0)\n",
      "Collecting protobuf<7,>=3.20 (from streamlit)\n",
      "  Downloading protobuf-6.32.0-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /Users/kavehnaini/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages (from streamlit) (21.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /Users/kavehnaini/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages (from streamlit) (2.32.5)\n",
      "Collecting tenacity<10,>=8.1.0 (from streamlit)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /Users/kavehnaini/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages (from streamlit) (4.15.0)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Downloading gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /Users/kavehnaini/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages (from streamlit) (6.5.2)\n",
      "Requirement already satisfied: jinja2 in /Users/kavehnaini/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /Users/kavehnaini/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
      "Collecting narwhals>=1.14.2 (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit)\n",
      "  Downloading narwhals-2.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/kavehnaini/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/kavehnaini/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/kavehnaini/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/kavehnaini/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kavehnaini/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kavehnaini/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kavehnaini/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kavehnaini/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/kavehnaini/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/kavehnaini/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/kavehnaini/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/kavehnaini/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/kavehnaini/miniconda3/envs/pitcherinjury/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Downloading streamlit-1.49.1-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m6m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m731.2/731.2 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading cachetools-6.2.0-py3-none-any.whl (11 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading protobuf-6.32.0-cp39-abi3-macosx_10_9_universal2.whl (426 kB)\n",
      "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading narwhals-2.2.0-py3-none-any.whl (401 kB)\n",
      "Installing collected packages: toml, tenacity, smmap, protobuf, narwhals, click, cachetools, blinker, pydeck, gitdb, gitpython, altair, streamlit\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13/13\u001b[0m [streamlit]13\u001b[0m [streamlit]\n",
      "\u001b[1A\u001b[2KSuccessfully installed altair-5.5.0 blinker-1.9.0 cachetools-6.2.0 click-8.2.1 gitdb-4.0.12 gitpython-3.1.45 narwhals-2.2.0 protobuf-6.32.0 pydeck-0.9.1 smmap-5.0.2 streamlit-1.49.1 tenacity-9.1.2 toml-0.10.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2600564e-a361-4819-be2e-76a8321b9910",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3737097518.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[25], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    streamlit run app.py\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "streamlit run app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2924a791-4274-44fa-9da9-0295452807f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3737097518.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    streamlit run app.py\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "streamlit run app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72bda13d-1849-4e58-81f3-a5e74334754b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streamlit version: 1.49.1\n",
      "Python: /Users/kavehnaini/miniconda3/envs/pitcherinjury/bin/python3.10\n",
      "\n",
      "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://192.168.86.21:8501\u001b[0m\n",
      "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://172.91.138.225:8501\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  For better performance, install the Watchdog module:\u001b[0m\n",
      "\n",
      "  $ xcode-select --install\n",
      "  $ pip install watchdog\n",
      "            \u001b[0m\n",
      "Gathering player lookup table. This may take a moment.\n",
      "2025-08-31 13:02:41.957 Please replace `use_container_width` with `width`.\n",
      "\n",
      "`use_container_width` will be removed after 2025-12-31.\n",
      "\n",
      "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
      "2025-08-31 13:02:45.875 Please replace `use_container_width` with `width`.\n",
      "\n",
      "`use_container_width` will be removed after 2025-12-31.\n",
      "\n",
      "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
      "2025-08-31 13:02:46.260 Please replace `use_container_width` with `width`.\n",
      "\n",
      "`use_container_width` will be removed after 2025-12-31.\n",
      "\n",
      "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
      "2025-08-31 13:02:46.416 Please replace `use_container_width` with `width`.\n",
      "\n",
      "`use_container_width` will be removed after 2025-12-31.\n",
      "\n",
      "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
      "2025-08-31 13:02:46.607 Please replace `use_container_width` with `width`.\n",
      "\n",
      "`use_container_width` will be removed after 2025-12-31.\n",
      "\n",
      "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
      "2025-08-31 13:02:46.771 Please replace `use_container_width` with `width`.\n",
      "\n",
      "`use_container_width` will be removed after 2025-12-31.\n",
      "\n",
      "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
      "2025-08-31 13:02:46.919 Please replace `use_container_width` with `width`.\n",
      "\n",
      "`use_container_width` will be removed after 2025-12-31.\n",
      "\n",
      "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
      "2025-08-31 13:02:51.722 Please replace `use_container_width` with `width`.\n",
      "\n",
      "`use_container_width` will be removed after 2025-12-31.\n",
      "\n",
      "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
      "2025-08-31 13:02:52.284 Please replace `use_container_width` with `width`.\n",
      "\n",
      "`use_container_width` will be removed after 2025-12-31.\n",
      "\n",
      "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
      "2025-08-31 13:02:52.419 Please replace `use_container_width` with `width`.\n",
      "\n",
      "`use_container_width` will be removed after 2025-12-31.\n",
      "\n",
      "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
      "2025-08-31 13:02:52.587 Please replace `use_container_width` with `width`.\n",
      "\n",
      "`use_container_width` will be removed after 2025-12-31.\n",
      "\n",
      "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
      "2025-08-31 13:02:52.735 Please replace `use_container_width` with `width`.\n",
      "\n",
      "`use_container_width` will be removed after 2025-12-31.\n",
      "\n",
      "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
      "2025-08-31 13:02:52.921 Please replace `use_container_width` with `width`.\n",
      "\n",
      "`use_container_width` will be removed after 2025-12-31.\n",
      "\n",
      "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
      "2025-08-31 13:02:53.068 Please replace `use_container_width` with `width`.\n",
      "\n",
      "`use_container_width` will be removed after 2025-12-31.\n",
      "\n",
      "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
      "2025-08-31 13:02:53.219 Please replace `use_container_width` with `width`.\n",
      "\n",
      "`use_container_width` will be removed after 2025-12-31.\n",
      "\n",
      "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
      "2025-08-31 13:02:53.387 Please replace `use_container_width` with `width`.\n",
      "\n",
      "`use_container_width` will be removed after 2025-12-31.\n",
      "\n",
      "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
      "2025-08-31 13:02:53.670 Please replace `use_container_width` with `width`.\n",
      "\n",
      "`use_container_width` will be removed after 2025-12-31.\n",
      "\n",
      "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
      "2025-08-31 13:03:01.523 Please replace `use_container_width` with `width`.\n",
      "\n",
      "`use_container_width` will be removed after 2025-12-31.\n",
      "\n",
      "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
      "2025-08-31 13:03:01.745 Please replace `use_container_width` with `width`.\n",
      "\n",
      "`use_container_width` will be removed after 2025-12-31.\n",
      "\n",
      "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
      "2025-08-31 13:03:01.940 Please replace `use_container_width` with `width`.\n",
      "\n",
      "`use_container_width` will be removed after 2025-12-31.\n",
      "\n",
      "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
      "2025-08-31 13:03:04.704 Please replace `use_container_width` with `width`.\n",
      "\n",
      "`use_container_width` will be removed after 2025-12-31.\n",
      "\n",
      "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
      "^C\n",
      "\u001b[34m  Stopping...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Run Streamlit from the notebook (served at http://localhost:8501)\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd().parent if Path.cwd().name == \"data\" else Path.cwd()\n",
    "app_path = ROOT / \"app.py\"\n",
    "assert app_path.exists(), f\"app.py not found at {app_path}\"\n",
    "\n",
    "# Optional: sanity check\n",
    "try:\n",
    "    import streamlit as st\n",
    "    print(\"Streamlit version:\", st.__version__)\n",
    "    print(\"Python:\", sys.executable)\n",
    "except Exception:\n",
    "    raise SystemExit(\"Streamlit not installed. Run: pip install streamlit\")\n",
    "\n",
    "# Launch the app (cell will show logs; stop with the stop button or restart kernel)\n",
    "!python -m streamlit run \"{app_path}\" --server.headless true --server.port 8501\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eab50e6c-dd9e-482b-8543-c28410836743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote .gitignore at: /Users/kavehnaini/pitcher-injury-predictor/.gitignore\n"
     ]
    }
   ],
   "source": [
    "# === (G6) Write a Python .gitignore (then follow terminal steps below) ===\n",
    "from pathlib import Path\n",
    "ROOT = Path.cwd().parent if Path.cwd().name==\"data\" else Path.cwd()\n",
    "gitignore = ROOT / \".gitignore\"\n",
    "gitignore.write_text(\"\"\"\n",
    "# Byte-compiled / cache\n",
    "__pycache__/\n",
    "*.py[cod]\n",
    "*.ipynb_checkpoints/\n",
    ".ipynb_checkpoints/\n",
    "\n",
    "# Environments\n",
    ".env\n",
    ".venv\n",
    "venv/\n",
    "ENV/\n",
    "env/\n",
    ".conda/\n",
    "*.egg-info/\n",
    "\n",
    "# Jupyter\n",
    "*/.ipynb_checkpoints/*\n",
    "\n",
    "# OS\n",
    ".DS_Store\n",
    "\n",
    "# Data/model artifacts (keep CSVs you want to share)\n",
    "data/*.csv.gz\n",
    "data/*.joblib\n",
    "models/\n",
    "*.log\n",
    "\n",
    "# Optional: include final CSVs and PDF\n",
    "!data/predictions_2023_*.csv\n",
    "!data/predictions_2023_*_top25.csv\n",
    "!data/pitcher_injury_summary.pdf\n",
    "!data/RESULTS_AND_LIMITATIONS.md\n",
    "!data/METHODS.md\n",
    "!data/COMMON_APP_LINE.txt\n",
    "\"\"\".lstrip())\n",
    "print(\"Wrote .gitignore at:\", gitignore.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294de70a-10a5-4be0-9b34-26b48caf9cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
